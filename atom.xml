<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuuzhao</title>
  
  <subtitle>Yuyu Zhao&#39;s Personal Website</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-29T09:13:42.541Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yuyu Zhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>English-句子翻译</title>
    <link href="http://yoursite.com/2019/03/28/ENGLISH-%E5%8F%A5%E5%AD%90%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/03/28/ENGLISH-句子翻译/</id>
    <published>2019-03-28T12:06:50.000Z</published>
    <updated>2019-03-29T09:13:42.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文中摘抄的句子"><a href="#论文中摘抄的句子" class="headerlink" title="论文中摘抄的句子"></a>论文中摘抄的句子</h1><p>In addition, an interesting phenomena is observed that objects in the same categories have high response on the same channels while responses of the rest channels are suppressed.</p><pre><code>- 我的翻译：此外， 我们还观察到一个有趣的现象， 相同类别的目标在相同的通道中有高相应，当其他通道被抑制时。- 译文：相同类别的对象在相同通道上具有较高的响应，而其余通道的响应则被抑制。- 思考：</code></pre><p>We propose a depth-wise separable correlation structure to enhance the cross-correlation to produce multiple similarity maps associated with different semantic<br>meanings.</p><pre><code>- 我的翻译：提出了一个基于深度可分离关系结构， 来增强互关性以产生多相似度图联系， - 译文：    我们提出了一种深度可分相关结构来增强互相关性，生成多个与不同语义相关的相似映射。</code></pre><p>The seminal work by Bolme et al. [3] introduces the Convolution Theorem from the signal processing field into visual tracking and transforms the object template matching problem into a correlation operation in the frequency domain.</p><p>These trackers formulate visual tracking as a cross-correlation problem and are expected to better leverage the<br>merits of deep networks from end-to-end learning.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;论文中摘抄的句子&quot;&gt;&lt;a href=&quot;#论文中摘抄的句子&quot; class=&quot;headerlink&quot; title=&quot;论文中摘抄的句子&quot;&gt;&lt;/a&gt;论文中摘抄的句子&lt;/h1&gt;&lt;p&gt;In addition, an interesting phenomena is obser
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Tracking-SiamRpn++</title>
    <link href="http://yoursite.com/2019/03/28/TRACKING-SIAMRPN/"/>
    <id>http://yoursite.com/2019/03/28/TRACKING-SIAMRPN/</id>
    <published>2019-03-28T02:01:35.000Z</published>
    <updated>2019-03-29T10:37:38.825Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tracker-DeepLearning"><a href="#Tracker-DeepLearning" class="headerlink" title="Tracker-DeepLearning"></a>Tracker-DeepLearning</h1><h2 id="SiamRPN"><a href="#SiamRPN" class="headerlink" title="SiamRPN++"></a>SiamRPN++</h2><h3 id="基于Siamese-网络的跟踪器"><a href="#基于Siamese-网络的跟踪器" class="headerlink" title="基于Siamese 网络的跟踪器"></a>基于Siamese 网络的跟踪器</h3><ul><li>将跟踪构想为 一个目标模板和一个搜索区域之间的卷及特征相互关系。 </li><li>与最先进的算法相比，依然存在精度差</li><li>不能利用来自深层神经网络的特征，如比ResNet-50更深层的网络</li></ul><h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><ul><li>证明了核心原因在于： 缺乏严格的平移不变性（the lack of strict translation invariance）</li><li>本文通过一个简单有效的空间感知抽样策略（spatial aware sampling strategy）突破了这个限制。</li><li>本文成功训练了一个 ResNet-driven Siamese tracker，对性能有显著提升。</li><li>提供了一个新的模型结构来执行分层和深度聚合，这不仅提高了准确率，还减少了模型大小。</li><li>实验结果， 在当前在5个最大的目标追踪基准数据集上， 我们获得了最好结果。 </li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>目标检测目前的挑战</p><ul><li>光照变化</li><li>遮挡</li><li>背景杂乱</li></ul></li><li><p>目前方法</p><ul><li><p>Siamese network based trackers</p><ul><li>将视觉目标跟踪问题看成：通过目标模板和搜索区域的特征表示的相关性 来 学习一个通用的相似性映射。</li><li>为了确保跟踪效率， 离线学习Siamese相似性函数通常在运行时是固定的。 </li></ul></li><li><p>CFNet tracker &amp; DSiame tracker </p><ul><li>分别通过一个运行中的平均模板，和一个快速转换模块，更新追踪模块。</li></ul></li><li><p>SiameRNN Tracker</p><ul><li>在Siamese network之后介绍了区域推荐网络（region proposal network）, 然后执行联合了分类和回归来跟踪。</li></ul></li><li><p>DaSiameRPN tracker</p><ul><li>进一步介绍了一个干扰器模块（distractor-aware module）,提高了模型的识别能力</li></ul></li><li><p>目前方法的缺陷</p><ul><li>在 精度和速度的平衡上， 即使是最好表现的Siamese 跟踪器， 在精度上依然与目前最好的方法存在显著差距。</li><li>这些方法都的网络结构都在类似于AlexNet的网络上构建了自己的网络来训练一个Siamese跟踪器</li><li>这些方法都多次尝试更复杂的架构，如ResNet ， 但是性能并不好</li></ul></li><li><p>分析原因</p><ul><li>本文发现关键问题来自 “绝对平移不变性”的破坏。（the strict translation invariance）</li><li>由于目标可能出现在搜索区域的任何位置， 目标模板的特征表示应该保持空间不变性（spatial invariant）</li><li>我们发现， 在新的深层结构中， 只有AlexNet 的 zero-padding variant 才能满足这个空间不变性约束。</li></ul></li></ul></li><li><p>本文为了解除这个约束</p><ul><li>提出了一个简单但有效的 采样策略（sampling strategy）来打破Siamese 跟踪器的空间不变性限制。 </li><li>本文成功训练了一个 SiamRPN 跟踪器，使用 ResNet 作为主干网络（backbone network）， 得到了显著的性能提高。</li><li>得益于ResNet 的结构， 我们提出了一个基于层的互相关运算特征聚合结构（layer-wise feature aggravation structure for the cross-correlatio operation）, 帮助跟踪器从多层次特征（multiple levels）中预测相似度映射（the similarity map）</li><li>通过分析 Siamese 网络结构的互相关性（cross-correlations）,我们发现他的两个网络分支在参数数量上高度不平衡。 </li><li>因此我们进一步提出了一种基于深度的分离关系结构， 它不仅大大减少了目标模板中的参数数量， 也使得整个模型的训练过程更加稳定。 </li><li>此外， 我们还观察到一个有趣的现象， 相同类别的对象在相同通道上具有较高的响应，而其余通道的响应则被抑制。</li><li>正交特性（The orthogonal property）也提高了跟踪性能</li></ul></li><li><p>总结，本文的主要贡献在以下四个方面</p><ul><li>提供了一个Siameses 跟踪器的深度分析，证明了当使用深层网络时准确率的下降原因来自 严格平移不变性的破坏。 </li><li>提出了一个简单但高效的采样策略， 来打破空间不变性限制，成功地训练了基于ResNet 结构的 Siamese 跟踪器</li><li>提出了一种基于层次的互相关操作特征聚合结构，这帮助跟踪器从多层次上学的的特征来预测相似度图。 </li><li>我们提出了一个深度可分离的相关结构来增强互相关，从而产生与不同语义相关的多重相似度图。</li></ul></li><li><p>实验效果</p><ul><li><p>测试基准：5个最大的跟踪基准（benchmarks）</p><ul><li>OTB2015</li><li>VOT2018</li><li>UAV123</li><li>LaSOT</li><li>TrackingNet</li></ul></li><li><p>测试效果： 35FPS</p></li></ul></li><li><p>此外，我们还提出了一种基于MobileNet 主干的快速跟踪器， 保持了可竞争的性能，训练速度70FPS. </p></li></ul><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul><li><p>Recent trackers (focus on the Siamese)</p><ul><li>[3]  将信号处理领域的卷积理论引入到视觉跟踪，将目标模板匹配问题转化为频域（frequency domain）的相关运算。</li><li>根据这种转换， 基于相关性的跟踪器不仅得到了搞笑的运行速度， 如果使用了合适的特征也会提高准确性[16 50 51 8 6 ]</li><li>深度学习模型广泛应用在视觉跟踪，跟踪算法基于深度特征表示到的相关滤波方法，[9 5]在留下的跟踪基准和比赛中占据了目前最先进的准确率。 </li><li>近期， 基于Siamese网络的跟踪器因为其在跟踪准确性和效率上的平衡， 取得了重要关注。  [40, 1, 15, 42, 41, 12, 24, 43,<br>52, 44]</li><li>这些跟踪器 将 视觉跟踪问题视作 互关系问题（cross-correlation problem），并且期望从端到端学习中利用深度网络的优势</li><li>为了从两个分支的 交叉关系中创作一个相似度图，他们训练了一个加入了两个神经分支的 Y形神经网络，一个是为了目标模板( template )，另一个是为了搜索区域（search region） </li><li><p>此外， 这些两个分支在训练阶段可以保持固定[40, 1, 15, 43, 24, 52]，或者在线更新来适应目标的外观变化。[42,41,12]</p><ul><li>42</li><li>41</li><li>12</li></ul></li><li><p>目前最好的Siamese 跟踪器 通过在Siamese网络后面增加一个区域推荐网络（Region proposal network）增强了跟踪效果,并且取得了很好结果。 [24,52]</p></li><li><p>但是在OTB基准上， 他们的追踪准确率与最先进的深度追踪器（ECO[5]）(MDNet 32)依然存在较大差距</p><ul><li>ECO [5]</li><li>MDNet [32]</li></ul></li></ul></li><li><p>Recent developments of deep architectures</p><ul><li>AlexNet [23] 在2012年被提出， 这个网络结构的研究迅速成长并且很多复杂的深度网络被提出了。</li><li>VGGNet[37] </li><li>GoogleNet[38]</li><li>ResNet[14]</li><li>MobileNet[18]</li><li>这些深度结构不仅在神经网络的设计上提供了更深的理解， 也推动了很多计算机视觉任务的发展，如目标检测[33]， 图像分割[4]， 和人体姿势估计[39]。</li><li>在深度视觉跟踪器中， 网络结构通常包括不超过5个由AlexNet 或者 VGGNet定制构成层。 </li><li>这个现象被解释为 浅层特征（shallow features）最有助于目标的准确定位。 </li><li>在本文的工作中，我们讨论了Siamese跟踪器可以有效得到提高，如果模型在整个孪生<br>网络中得到适当的训练，那么使用更深层次的模型，孪生追踪器的性能可以显著提高。</li></ul></li><li><p>benchmark datasets</p><ul><li>[45, 46, 19, 21, 10, 30] </li><li>45 46 10 : 为不同算法的比较提供了公平实验台</li><li>每年一次的跟踪挑战（22, 19,20， 21），持续不断推动跟踪算法的发展</li></ul></li><li><p>methodologies</p><ul><li>[16,51, 6, 7, 17, 32, 9, 5, 43, 52, 49].</li></ul></li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Tracker-DeepLearning&quot;&gt;&lt;a href=&quot;#Tracker-DeepLearning&quot; class=&quot;headerlink&quot; title=&quot;Tracker-DeepLearning&quot;&gt;&lt;/a&gt;Tracker-DeepLearning&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文检索方法汇总</title>
    <link href="http://yoursite.com/2019/03/24/%E8%AE%BA%E6%96%87%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2019/03/24/论文检索方法汇总/</id>
    <published>2019-03-24T02:14:23.000Z</published>
    <updated>2019-03-25T08:40:28.608Z</updated>
    
    <content type="html"><![CDATA[<h2 id="各方向期刊特点、兴趣、引用因子、影响力"><a href="#各方向期刊特点、兴趣、引用因子、影响力" class="headerlink" title="各方向期刊特点、兴趣、引用因子、影响力"></a>各方向期刊特点、兴趣、引用因子、影响力</h2><p>顶级会议：</p><p>期刊：</p><h2 id="如何评判期刊的好坏？"><a href="#如何评判期刊的好坏？" class="headerlink" title="如何评判期刊的好坏？"></a>如何评判期刊的好坏？</h2><pre><code>- 指标    - 期刊兴趣        -特定学科领域内，期刊可能只关注其中某一方面        - 如果文章华而不实，投letter        - 如果文章理论扎实，应用面面俱到， 投full paper    - 期刊关注的研究类型        - 理论型？        - 应用型?    - 影响因子    - 刊载能力    - IF    - 拒稿率    - 每月在线文章量        - 在线的文章，是将要正式出版已经校样的稿件    - 每期文章量    - 每年多少期    - 收稿日期    - 正式出版日期    - 审稿日期    - 要求作者resubmission    - 发表周期        - 审稿周期            - 每年出版多少期， 月刊的评审周期可能明显短于年刊            - 从侧面了解期刊对审稿人的要求，邀请审稿信中是否希望3-5天决定能否审稿，如果20天左右返回审稿意见，如果拒稿最好10天内返回意见，是负责不错的期刊。        -     - 期刊主编        - 机构主页，个人主页        - 研究兴趣        - 团队是否有专职助手 （这样的杂志稿件处理进程非常正常）    - 副主编负责制的期刊，投稿时需要推荐        - 投稿时，和Cover Letter中需要选择推荐副主编。    - 期刊的办事规则和评价体系        - 如果用的不是推荐审稿人，是数据库审稿人（刚刚投了就接二连三收到审稿邀请）    - 读者群和目标读者        - 综合性期刊：非专业读者        - 专业期刊： 技术性， 专业性，少量特定读者    - 期刊的关注度        - 其他研究者对期刊的关注程度        - 是否被纳入电子数据库        - 是否被纳入本领域的热门专题数据库        - 期刊是否可在线阅读    - 发表CV值        - 编辑委员会成员        - 期刊赞助者     - </code></pre><h3 id="期刊的刊载能力"><a href="#期刊的刊载能力" class="headerlink" title="期刊的刊载能力"></a>期刊的刊载能力</h3><pre><code>- 1、关注近几年的IF， 如果在2－3之间跳跃性振荡，忽涨忽跌，学术性要求不是很高很专，趣味广泛。</code></pre><h2 id="如何挑选论文？"><a href="#如何挑选论文？" class="headerlink" title="如何挑选论文？"></a>如何挑选论文？</h2><h2 id="如何选择投稿期刊？"><a href="#如何选择投稿期刊？" class="headerlink" title="如何选择投稿期刊？"></a>如何选择投稿期刊？</h2><pre><code>- 副主编选择    - 年轻的中国和日本牛人是工作狂    - 如果文章存在硬伤，要权衡副主编的学术和工作效率，取平衡    - 如果文章基本理论不扎实， 亮点是方法新，结果好， 千万不要推荐给经常发表6pages以上文章的专业副主编。      最好选择与自己文章方向稍微擦边的其他学科的人来负责。     - 如果工作的核心是细致的工作， 选择经常知道学生做类似工作的编辑。 - 自己写推荐审稿人    - 只要推荐的审稿人满足一定条件    - 推荐的审稿人如果曾经在投送期刊上发表过文章， 编辑更容易根据他注册时提供的信息比对    - 专业上小有名气的学者    - 选择推荐审稿人时考虑：        - 1、方向接近        - 2、水平不一过于悬殊，如果投的期刊很一般，自己的学术出生不耀眼，特别牛气的人很可能拒审        - 3、核实同学e-mail 是否是本人的信箱。        - 4、回避有积怨的group        - 5、审稿人的背景，如果学科领域有和蔼仁厚的长者，（从会议、其周围的人、学术圈前辈等去了解）， 得益于他们的学术影响力， 如果其他审稿人的意见不是特别尖锐，就容易通过了。         - 6、审稿人的精力充沛，（社会兼职少，行政头衔少，负责的课题组建立时间不长但发展态势较好）- 期刊是否有猫腻（一些课题组为了博士可以毕业）        - 鉴别方法：看看这个领域是否有较牛的课题组，每年有4篇以上的文章发表， 同时在同一期刊上有不错的IF和论文出版量。 - 每个期刊都有自己的圈子， 如果自己的文章不符合圈内人的说话方式，学术贡献又不是凤毛麟角，被拒就是很正常的</code></pre><h2 id="期刊分区"><a href="#期刊分区" class="headerlink" title="期刊分区"></a>期刊分区</h2><pre><code>中科院期刊分期，把期刊分成4个档次（也称4个区）</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;各方向期刊特点、兴趣、引用因子、影响力&quot;&gt;&lt;a href=&quot;#各方向期刊特点、兴趣、引用因子、影响力&quot; class=&quot;headerlink&quot; title=&quot;各方向期刊特点、兴趣、引用因子、影响力&quot;&gt;&lt;/a&gt;各方向期刊特点、兴趣、引用因子、影响力&lt;/h2&gt;&lt;p&gt;顶级
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tips</title>
    <link href="http://yoursite.com/2019/03/23/TIPS/"/>
    <id>http://yoursite.com/2019/03/23/TIPS/</id>
    <published>2019-03-23T12:47:20.000Z</published>
    <updated>2019-03-25T01:54:44.817Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何在两分钟内快速睡眠"><a href="#如何在两分钟内快速睡眠" class="headerlink" title="如何在两分钟内快速睡眠"></a>如何在两分钟内快速睡眠</h2><pre><code>1、 放松脸部肌肉， 注意力集中在额头中心，感受额头是平的2、肩膀完全放平， 接着放松手臂， 手掌， 手指。3、 吐气， 4、放松床脚， 从上而下，最后注意力放在脚掌5、放松精神， 享受自己在独木舟上， 周围是湖面， 蓝天白云    在蓝色吊床上， 对自己说， 放空，放空，放空。六周训练~！</code></pre><h2 id="简单的心理学技巧"><a href="#简单的心理学技巧" class="headerlink" title="简单的心理学技巧"></a>简单的心理学技巧</h2><pre><code>神经语言规划+ 心理学1、强制要求： 麻烦你，做XXX。 有礼貌： 2、盯着额头会让人3、沉默一下，保持目光接触， 可以让对方透露更多。 4、新环境， 问对方问题， 即使知道。 5、想要得到肯定回复， 提问时，微微点头。 6、在别人集中注意力做一件事情的时候， 这个时候对方的注意力不着急。 7、激将法， 质疑对很好做一件事情的能力， 提一个小小的要求，可以建立联系。 8、和人交谈时， 一边说话一边点头，可以让对方愿意和你说话。9、谈判时，答应时表示一点失望。10、忘记做一些事情的时候， 就说一些奇怪的名词， 然后再做这些事情。 11、和对方争吵时， 对对方说一些好话。 避免争吵可以保持沉默。 12、 交代任务说 ， 从这个开始做，而不是把这个做了。 13、演讲时带一瓶水，紧张时喝一口。 14、 如果陌生人盯着你看，就把视线放在别人的鞋子上。 15、 和人看到的时候表示很不安的话， 对方每次看到自己也会很不安。 16、 指责对方时， 不直接说对方，而是陈述客观事实，不带人。 17、如果没有睡好，就对自己暗示， 我睡得很好！ 让自己相信， 大脑就很好。19，人们对一天早上和晚上发生的事情印象比较深刻。 早上：     不能按下贪睡按钮，一鼓作气爬起来。 大脑会进入昏昏沉沉的惰性。         早上拒绝查看社交媒体动态！        早上到早饭前都不要看手机！        起床后不喝咖啡，喝白开水。         1:1：1饮食， 喝室温温度的水 ！        整理床铺， 是在完成一个健康的仪式。        早上不摄取毫无价值的糖和碳水化合物， 拒绝糖分居多的，挑选蛋白质丰富的食物。        早起开窗帘，查看光线清醒，自然光分泌皮质醇。         早上伸展身体，慢慢做一些伸展运动，帮助肌肉和椎间盘开始运动。         早上9:30以后才打电话。 情感是人与人纽带的控制。    人在抚养期间要产生情感。     真正能拿住人的是情感。    追逐表象，容易迷失。    万变不离其宗， 潜意识。###黑格尔哲学    要找出所有流派出现和存在的价值。     前人的失败是后人进步的基础。    批判和抛弃有关，找出合理价值， 学习对手的优点。    从认识角度，物质和精神是同时存在的。    真正拿住人的是情感，情感是养出来的。    人应该顺应自然，而不是改善自然。    尊重自然，研究自然背后的精神。    气愤，化解自己不良情绪， 听音乐， 运动， 娱乐化解一时刺激应发的情绪。    人要学会反省，学会做一些弥补性的问题。    6岁之前的生活经历。     吃得苦越多， 承受力越强。     生活早年的苦难。    培养体力。    知道一个点后面做，通读相关工作论文，    research perpose . ** + 实验 +引言平时积累， 每天读到的东西    读懂了一篇文章，基于这篇文章，写几句话简单介绍一下。    有一些新的想法， 意义，一两句话，比如图像识别很有意义，挑战是什么，我要解决的是哪个挑战，哪个意义，每一段时间写一点，去找文章确定没人做，调研阶段，每个文献都有记录，expose, 自己写了一个word,哪一年，哪些会议，哪些人，做了哪些工作，一边记录，一边编代码，    抓住一个点，怎么减少第一次学习需要的样本量，抓住一个点，文章里面怎么解决的，然后记录。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;如何在两分钟内快速睡眠&quot;&gt;&lt;a href=&quot;#如何在两分钟内快速睡眠&quot; class=&quot;headerlink&quot; title=&quot;如何在两分钟内快速睡眠&quot;&gt;&lt;/a&gt;如何在两分钟内快速睡眠&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;1、 放松脸部肌肉， 注意力集中在额头中心，感受额头
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>热键</title>
    <link href="http://yoursite.com/2019/03/23/%E7%83%AD%E9%94%AE/"/>
    <id>http://yoursite.com/2019/03/23/热键/</id>
    <published>2019-03-23T12:29:32.000Z</published>
    <updated>2019-03-23T12:46:20.436Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建虚拟桌面"><a href="#创建虚拟桌面" class="headerlink" title="创建虚拟桌面"></a>创建虚拟桌面</h2><pre><code>CTRL + WIN + D切换  win + ctrl +  左右关闭    win + ctrl + f4 </code></pre><h2 id="单桌面切换"><a href="#单桌面切换" class="headerlink" title="单桌面切换"></a>单桌面切换</h2><pre><code>win + m  全部最小化win + , </code></pre><h2 id="任务栏：-win-1-2-…9"><a href="#任务栏：-win-1-2-…9" class="headerlink" title="任务栏： win + 1..2.. …9"></a>任务栏： win + 1..2.. …9</h2><h2 id="分屏：-win-左右"><a href="#分屏：-win-左右" class="headerlink" title="分屏：  win + 左右"></a>分屏：  win + 左右</h2><h2 id="切换任务：-ctrl-win-tab"><a href="#切换任务：-ctrl-win-tab" class="headerlink" title="切换任务： ctrl + win + tab"></a>切换任务： ctrl + win + tab</h2><pre><code>win + tab alt + tab</code></pre><h2 id="打开任务管理器，强制关闭窗口：-ctrl-shift-esc"><a href="#打开任务管理器，强制关闭窗口：-ctrl-shift-esc" class="headerlink" title="打开任务管理器，强制关闭窗口： ctrl + shift + esc"></a>打开任务管理器，强制关闭窗口： ctrl + shift + esc</h2><h2 id="打开文档管理器：-win-E"><a href="#打开文档管理器：-win-E" class="headerlink" title="打开文档管理器： win + E"></a>打开文档管理器： win + E</h2><h2 id="使用放大镜：-win"><a href="#使用放大镜：-win" class="headerlink" title="使用放大镜： win + +/-"></a>使用放大镜： win + +/-</h2><h2 id="捕捉屏幕：-win-prin"><a href="#捕捉屏幕：-win-prin" class="headerlink" title="捕捉屏幕：  win + prin.."></a>捕捉屏幕：  win + prin..</h2><h2 id="操作中心打开：-win-A"><a href="#操作中心打开：-win-A" class="headerlink" title="操作中心打开： win + A"></a>操作中心打开： win + A</h2><h2 id="进入设置：-win-i"><a href="#进入设置：-win-i" class="headerlink" title="进入设置： win + i"></a>进入设置： win + i</h2><h2 id="与cortana对话：-win-c"><a href="#与cortana对话：-win-c" class="headerlink" title="与cortana对话： win + c"></a>与cortana对话： win + c</h2><h2 id="使用游戏栏：-win-G-手动录影"><a href="#使用游戏栏：-win-G-手动录影" class="headerlink" title="使用游戏栏： win + G 手动录影"></a>使用游戏栏： win + G 手动录影</h2><pre><code>启动停止： </code></pre><h2 id="锁住PC-：-win-L"><a href="#锁住PC-：-win-L" class="headerlink" title="锁住PC ： win + L"></a>锁住PC ： win + L</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建虚拟桌面&quot;&gt;&lt;a href=&quot;#创建虚拟桌面&quot; class=&quot;headerlink&quot; title=&quot;创建虚拟桌面&quot;&gt;&lt;/a&gt;创建虚拟桌面&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;CTRL + WIN + D

切换  win + ctrl +  左右

关闭    wi
      
    
    </summary>
    
    
      <category term="tips" scheme="http://yoursite.com/tags/TIPS/"/>
    
  </entry>
  
  <entry>
    <title>Tracker</title>
    <link href="http://yoursite.com/2019/03/22/TRACKER/"/>
    <id>http://yoursite.com/2019/03/22/TRACKER/</id>
    <published>2019-03-22T09:14:01.000Z</published>
    <updated>2019-03-22T15:43:24.639Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标跟踪方法调研"><a href="#目标跟踪方法调研" class="headerlink" title="目标跟踪方法调研"></a>目标跟踪方法调研</h1><h2 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h2><ul><li>前几名：Struck, SCM, ASLA，CSK, L1APG</li><li><img src="/2019/03/22/TRACKER/1.png" alt=""></li><li><img src="/2019/03/22/TRACKER/2.png" alt=""></li><li>OAB 2006</li><li>IVT 2008</li><li>MIL 2009</li><li>TLD 2010</li><li>STRUCK 2011</li><li>CT 2012</li><li>SCM 2012</li></ul><h2 id="目标跟踪面临的难点"><a href="#目标跟踪面临的难点" class="headerlink" title="目标跟踪面临的难点"></a>目标跟踪面临的难点</h2><ul><li>外观变形</li><li>光照变化</li><li>快速运动</li><li>运动模糊</li><li>背景相似干扰</li><li><p>我的思考</p><ul><li>相同的目标同时出现</li></ul></li><li><p>平面外旋转</p></li><li>平面内旋转</li><li>尺度变化</li><li>遮挡和离开视野</li></ul><h2 id="目前常用数据库"><a href="#目前常用数据库" class="headerlink" title="目前常用数据库"></a>目前常用数据库</h2><ul><li><p>OTB</p><ul><li>OTB包括25%的灰度序列</li><li>随机帧开始，或矩形框加随机干扰初始化</li></ul></li><li><p>VOT</p><ul><li>但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因</li><li>VOT库的序列分辨率普遍较高</li><li>VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后再次初始化</li><li>VOT2016，因为序列都是精细标注，且评价指标我更加认可(</li></ul></li></ul><h2 id="目标视觉跟踪方法分类"><a href="#目标视觉跟踪方法分类" class="headerlink" title="目标视觉跟踪方法分类"></a>目标视觉跟踪方法分类</h2><ul><li><p>生成模型方法</p><ul><li>在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置</li><li><p>著名方法</p><ul><li>卡尔曼滤波</li><li>粒子滤波</li><li>mean-shift</li></ul></li><li><p>举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域</p></li><li><p>推荐算法ASMS</p><ul><li><a href="https://github.com/vojirt/asms" target="_blank" rel="noopener">https://github.com/vojirt/asms</a></li><li>VOT2015的第20名，官方推荐的实时算法，VOT2016的32名，平均帧率125FPS，在经典mean-shift框架下加入了尺度估计，经典颜色直方图特征，加入了两个先验(尺度不剧变+可能偏最大)作为正则项，和反向尺度一致性检查，作者给了C++代码，在相关滤波和深度学习盛行的年代，还能看到mean-shift打榜，如此高的性价比实在不容易(已泪目~~)，实测性能还不错。</li></ul></li></ul></li><li><p>判别模型方法</p><ul><li>又叫检测跟踪tracking-by-detection</li><li>经典套路图像特征+机器学习</li><li><p>当前帧以目标区域为正样本，背景区域为负样本，机器学习训练分类器，下一帧用训练好的分类器找最优区域</p><ul><li><img src="/2019/03/22/TRACKER/3.png" alt=""></li></ul></li><li><p>分类器训练过程中用到了背景信息，这样分类器专注区分前景和背景，判别类方法普遍都比生成类好</p></li><li><p>相关滤波类方法</p><ul><li>别看那些五花八门的机器学习方法，那都是虚的，目标跟踪算法中特征才是最重要的。,对于那些仅简单修改机器学习方法而不关注如何减小边界效应的相关滤波，也许可能大概会有效果，</li></ul></li><li><p>深度学习(Deep ConvNet based)类方法</p><ul><li><p>MDNet</p><ul><li><a href="http://cvlab.postech.ac.kr/research/mdnet/" target="_blank" rel="noopener">http://cvlab.postech.ac.kr/research/mdnet/</a></li></ul></li><li><p>TCNN</p><ul><li><a href="http://www.votchallenge.net/vot2016/download/44_TCNN.zip" target="_blank" rel="noopener">http://www.votchallenge.net/vot2016/download/44_TCNN.zip</a></li></ul></li><li><p>SiamFC</p><ul><li><a href="http://www.robots.ox.ac.uk/~luca/siamese-fc.html" target="_blank" rel="noopener">www.robots.ox.ac.uk/~luca/siamese-fc.html</a></li></ul></li><li><p>SiamFC-R</p><ul><li><a href="http://www.iqiyi.com/w_19ruirwrel.html#vfrm=8-8-0-1" target="_blank" rel="noopener">http://www.iqiyi.com/w_19ruirwrel.html#vfrm=8-8-0-1</a></li></ul></li><li><p>GOTURN</p><ul><li><a href="https://github.com/davheld/GOTURN" target="_blank" rel="noopener">https://github.com/davheld/GOTURN</a></li></ul></li></ul></li></ul></li></ul><h2 id="基于孪生网络的方法汇总"><a href="#基于孪生网络的方法汇总" class="headerlink" title="基于孪生网络的方法汇总"></a>基于孪生网络的方法汇总</h2><ul><li><img src="/2019/03/22/TRACKER/4.png" alt="Recent Tracher develop"></li><li><p>SINT</p><ul><li><p>首次开创性的将目标跟踪问题转化为一个patch块匹配问题，并神经网络来实现。</p><ul><li><img src="/2019/03/22/TRACKER/6.png" alt="SINT"></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取ALOV数据集，并进行预处理；</li><li><p>步骤2：搭建如图所示的类似AlexNet的网络架构（引入ROI Pooling层）；</p><ul><li><img src="/2019/03/22/TRACKER/5.png" alt="SINT"></li></ul></li><li><p>步骤3：在训练数据中获取patch块，开始训练网络，获得最终的模型，即所谓的相似度函数；</p></li><li>步骤4：使用预训练好的网络进行跟踪，即所谓的patch块匹配，输出相应的结果。</li></ul></li><li><p>创新点</p><ul><li>创新1- 创造性的将目标跟踪任务转化为patch块匹配问题；</li><li>创新2-设计了相应的网络架构，尝试着去解决这个问题；</li></ul></li></ul></li><li><p>Siamese-fc</p><ul><li><p>这个框图与众不同的是它是一个端到端的跟踪网络，而且速度很快！这篇论文使得基于孪生网络的跟踪器火了起来，让研究者们看到了新的希望。</p><ul><li><img src="/2019/03/22/TRACKER/7.png" alt=""></li></ul></li><li><p>算法实现步骤</p><ul><li>步骤1：获取VID数据集，并进行预处理操作获得相应的patch块数据；</li><li>步骤2：将获取的patch数据输入到上图所示的网络架构中进行训练，从而获得最终的模型；</li><li>步骤3：将训练好的模型分别应用在视频中的第一帧和后续帧中，通过相关操作获得最终的结果；</li></ul></li><li><p>创新点</p><ul><li>创新1-设计了一个端到端的跟踪网络；</li><li>创新2-获得了一个很快的跟踪速度，Titan xp 58fps；</li><li>创新3-使用了一个更大的数据集来训练网络；</li><li>创新4-提到了很多关键的细节，包括网络中padding的影响，以及网络输入的大小等。</li></ul></li></ul></li><li><p>CFNet</p><ul><li><p>这篇文章整体思路和Siamese-fc算法的思路基本相同，不同之处在于将相关滤波（CF）整合为一个网络层，并将其嵌入到基于孪生网络的框架中，如图中蓝色的区域所示。</p><ul><li><img src="/2019/03/22/TRACKER/8.png" alt=""></li></ul></li><li><p>步骤同上</p></li><li><p>创新点</p><ul><li>创新1-将相关滤波操作变成了一个单一的网络层，并嵌入到网络中；</li><li>创新2-调整了网络的架构，比如输入等；</li><li>创新3-向我们展示了严格的理论推导过程，值得去学习！！！</li></ul></li></ul></li><li><p>DSiam</p><ul><li><blockquote><p>论文：<br><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf</a><br>代码：<a href="https://github.com/tsingqguo/DSiam" target="_blank" rel="noopener">https://github.com/tsingqguo/DSiam</a></p></blockquote></li><li><p>该算法在siamese-fc框架的基础上面添加了目标外观变换转换层和背景抑制变换层来提升网络的判别能力，即增强了模型在线更新的能力。</p><ul><li><img src="/2019/03/22/TRACKER/9.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取ILSVC-2015数据集（又名VID），并进行预处理操作；</li><li>步骤2：搭建如上图所示的网络模型，引入了circular convolution (‘CirConv’) and regularized linear regression (‘RLR’)层，进行模型的训练过程；</li><li>步骤3：将预训练好的模型分别应用在第一帧和后续帧中，并进行模型在线更新操作，通过相关操作获取对应的得分映射，得到最终的结果。</li></ul></li><li><p>创新点</p><ul><li>创新1-为了提升模型的泛化能力，分别在x和z分支中外观变化转换层和背景抑制转换层；</li><li>创新2-尝试着融合基准网络的不同层；</li><li>创新3-严格的理论推导，值得学习。</li></ul></li></ul></li><li><p>SINT++</p><ul><li>论文：<br><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf</a><br>项目：<br><a href="https://sites.google.com/view/cvpr2018sintplusplus/" target="_blank" rel="noopener">https://sites.google.com/view/cvpr2018sintplusplus/</a><br>Poster：<br><a href="https://drive.google.com/file/d/1Tn-WkOM3gkCX7Upp5X-YELxUbLAE3o8a/view?usp=sharing" target="_blank" rel="noopener">https://drive.google.com/file/d/1Tn-WkOM3gkCX7Upp5X-YELxUbLAE3o8a/view?usp=sharing</a></li><li><p>这篇文章是SINT算法的改进版，主要的改进部分如图中篮框所示，目的是为了生成多样性的输入正样本块，使用到了AE(AutoEncoder)和GAN网络。</p><ul><li><img src="/2019/03/22/TRACKER/10.png" alt=""></li></ul></li><li><p>算法实现步骤</p><ul><li>步骤1：分别从VOT-2013、VOT-2014和VOT-2016中收集一部分数据集，进行预处理操作；</li><li>步骤2：将获取到的训练集依次输入到正样本块生成网络和正样本块转换网络中去生成多样性的这样本块；</li><li>步骤3：将获取的样本块输入到SINT网络中进行训练，获得相应的网络模型；</li><li>步骤4：将整个预训练网络分别应用在第一帧和后续帧中，获取最终的结果。</li></ul></li><li><p>创新点</p><ul><li>创新1-使用PSGN网络获得多样性的样本图像，从而提升了算法的鲁棒性；</li><li>创新2-使用HPTN网络来处理目标遮挡问题，提升算法的鲁棒性；</li><li>创新3-将AE和GAN这种比较热的概念应用到跟踪算法中，即所谓的应用创新。</li></ul></li></ul></li><li><p>SA-Siam</p><ul><li><p>该算法中的主要改动如图中绿框所示，即双网络分别学习不同的特征、在Z分支添加注意力机制和多层特征的融合。</p><ul><li><img src="/2019/03/22/TRACKER/11.png" alt=""></li></ul></li><li><p>论文：<br><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf</a></p></li><li><p>步骤</p><ul><li>步骤1：获取ILSVRC-2015中的彩色图片作为训练集，进行预处理操作；</li><li>步骤2：利用训练集数据分别训练两个不同的网络分支，即A-Net和S-Net，获得相应的网络模型；</li><li>步骤3：将整个网络整合在一起并将其应用在视频中获取相应的得分映射图；</li></ul></li><li><p>创新点</p><ul><li>创新1-使用两个网络分别获取网络的语义特征和外观特征；</li><li>创新2-对两个分支网络进行单独训练；</li><li>创新3-在语义分支网络中使用了注意力机制和特征融合；</li></ul></li></ul></li><li><p>RASNet</p><ul><li><p>该算法中最大的特色是将Attention机制应用的淋漓尽致，使用到了3个注意力机制，包括残差注意力块，通道注意力块和通用注意力块。</p><ul><li><img src="/2019/03/22/TRACKER/12.png" alt=""></li></ul></li><li><p>通过注意力机制可以使得整个网络可以根据目标的变化而自适应的进行调整。</p></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf</a></li><li><p>步骤</p><ul><li>步骤1：获取ILSVRC15数据集作为训练集，并进行数据预处理操作；</li><li>步骤2：搭建如上图所示的网络架构，分别训练获得残差注意力块和通道注意力块；</li><li>步骤3：将预训练的注意力模块应用到视频中，进行相关操作获得最终的结果。</li></ul></li><li><p>创新</p><ul><li>创新1-多个注意力机制使得网络不需要进行在线更新操作，其实更新操作换做注意力机制来做！</li><li>创新2-通过残差注意力机制和通道注意力机制在缓解网络过拟合的同时提升网络的判别能力；</li><li>创新3-考虑到视频中的时空信息，并通道注意力机制来获得。</li></ul></li></ul></li><li><p>SiamRPN</p><ul><li>论文：<br><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf</a><br>项目：<a href="http://bo-li.info/SiamRPN/" target="_blank" rel="noopener">http://bo-li.info/SiamRPN/</a><br>代码(PyTorch)：<br><a href="https://github.com/songdejia/Siamese-RPN-pytorch" target="_blank" rel="noopener">https://github.com/songdejia/Siamese-RPN-pytorch</a><br>代码(TensorFlow)：<br><a href="https://github.com/makalo/Siamese-RPN-tensorflow" target="_blank" rel="noopener">https://github.com/makalo/Siamese-RPN-tensorflow</a></li><li><p>将目标检测中的RPN模块应用到跟踪任务中来，即如图中绿色区域的分类和回归分支，由于回归分支的存在使得该算法可以去掉原始的尺度金字塔，因此该算法在提升精度的同时达到的提速。将原始的相似度计算问题转化为回归和分类问题。</p><ul><li><img src="/2019/03/22/TRACKER/13.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取VID和 Youtube-BB数据集作为训练数据集，并进行数据预处理；</li><li>步骤2：搭建如上图所示的网络框架，同时进行分类分支和回归分支的训练，获得网络模型；</li><li>步骤3：应用预训练的模型到视频中通过分类和回归操作初步获得目标；</li><li>步骤4：使用NMS等多个tricks进行后处理操作，获得最终的目标。</li></ul></li><li><p>创新点</p><ul><li>创新1-将RPN的思路应用到跟踪领域中，在提速的同时提升了精度；</li><li>创新2-引入1x1卷积层来对网络的通道进行升维处理；</li><li>创新3-尝试着去使用更大的数据集，光Youtube-BB就几千个视频序列，所以数据也是至关重要的！！</li><li>创新4-使得基于孪生网络的跟踪算法的性能有了大幅度的提升，让我们看到了希望！！！</li></ul></li></ul></li><li><p>SiamFC-tri</p><ul><li>论文：<br><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf</a><br>代码：<br><a href="https://github.com/shenjianbing/TripletTracking" target="_blank" rel="noopener">https://github.com/shenjianbing/TripletTracking</a></li><li><p>该文的主要工作是将孪生网络领域中使用广泛的triplet loss应用到跟踪问题上来，可以说是一个应用创新。</p><ul><li><img src="/2019/03/22/TRACKER/15.png" alt=""></li></ul></li><li><p>实现步骤和siamese-fc基本类似，增加了triplet分支块。</p></li></ul></li><li><p>StructSiam</p><ul><li><p>框架</p><ul><li><img src="/2019/03/22/TRACKER/14.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取ILSVRC VID数据集作为训练集，并进行预处理操作；</li><li>步骤2：搭建如上图所示的网络框架，需要关注的是局部模式检测层和纹理模型的构建，从而获得相应的模型；</li><li>步骤3：使用预训练模型通过相关操作获得一个更准确的得分映射图；</li><li>步骤4：进行后续的加窗等后处理操作；</li></ul></li><li><p>创新点</p><ul><li>创新1-使用局部结构学习方法来减缓模型对非刚体运动变化的敏感程度；</li><li>创新2-通过多个局部结构块的任意组合可以形成更丰富的纹理；</li><li>创新3-将相似性比较问题转化为一个局部特征块的比较问题。</li></ul></li></ul></li><li><p>DaSiamRPN</p><ul><li>论文：<br><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf</a><br>Silde：<br><a href="https://drive.google.com/file/d/1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr/view</a><br>代码：<br><a href="https://github.com/foolwood/DaSiamRPN" target="_blank" rel="noopener">https://github.com/foolwood/DaSiamRPN</a></li><li><p>该论文是对SiamRPN论文的初步改进版，主要解决的问题是正负样本块不均衡问题和样本块的丰富性问题，即更关注于输入数据的问题。</p><ul><li><img src="/2019/03/22/TRACKER/16.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>创新1-发现训练正负样本块不均衡问题会影响网络判别相似目标的能力，不如两个穿着黄色衣服的人；</li><li>创新2-引入一个高效的采样策略来解决该问题；</li><li>创新3-引入一个局部到全局的搜索策略来解决long-term跟踪问题，这个问题正在变得越来越重要！！！</li></ul></li></ul></li><li><p>DenseSiam</p><ul><li><p>论文：<a href="https://arxiv.org/abs/1809.02714" target="_blank" rel="noopener">https://arxiv.org/abs/1809.02714</a><br>代码：<br><a href="http://www.votchallenge.net/vot2018/trackers.html" target="_blank" rel="noopener">http://www.votchallenge.net/vot2018/trackers.html</a></p></li><li><p>该论文主要工作是将Dense-Block应用到跟踪网络中来，是一个类似于残差块的东西，不过它是密集型链接，该思路借鉴于图像分类，同时在目标图像分支增加了注意力模块提升模型的自适应能力。</p><ul><li><img src="/2019/03/22/TRACKER/17.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取ILSVRC1数据集作为训练数据集，进行数据预处理操作；</li><li>步骤2：搭建如上图所示的网络，关注于Dense=Block块的搭建细节和注意力机制的实现细节，获得网络模型；</li><li>步骤3：将预训练的模型应用到视频中获取一个好的score_map，获得初步结果；</li><li>步骤4：进行加窗等后处理操作获得最终结果。</li></ul></li><li><p>创新点</p><ul><li>创新1-使用Dense-Block来获取更鲁棒的特征表示；</li><li>创新2-构建了一个相对于AlexNet网络更深的一个网络；</li><li>创新3-添加注意力机制来提升得分相应的效果。</li></ul></li></ul></li><li><p>Siam-BM</p><ul><li><p>主要思路是通过角度评估模块和空间mask模块分别来解决目标的大尺度旋转和网络区分相似目标的能力。</p><ul><li><img src="/2019/03/22/TRACKER/18.png" alt=""></li></ul></li><li><p>论文：<a href="https://arxiv.org/abs/1809.01368" target="_blank" rel="noopener">https://arxiv.org/abs/1809.01368</a><br>项目：<a href="https://77695.github.io/Siam-BM" target="_blank" rel="noopener">https://77695.github.io/Siam-BM</a><br>代码：<a href="https://github.com/77695/Siam-BM" target="_blank" rel="noopener">https://github.com/77695/Siam-BM</a></p></li><li><p>步骤</p><ul><li>步骤1：获取ILSVRC-2015数据集中的彩色图片作为训练集，并进行预处理操作；</li><li>步骤2：构建上图的网络主要关注角度评估块和mask模块的构造，获得最终的网络模型；</li><li>步骤3：将预训练的模型应用到视频中，获取多个可能的得分映射图，选择最好的映射图作为初步结果；</li><li>步骤4：进行加窗等后处理操作，返回最终的结果。</li></ul></li><li><p>创新点</p><ul><li>创新1-针对siamese-fc中没有考虑到的角度评估问题提出了解决方案，尽量不影响算法速度；</li><li>创新2-根据目标的比例选择出合适的mask来提升算法区分相似目标的能力；</li></ul></li></ul></li><li><p>C-RPN</p><ul><li><p>主要思路是为了区分相似的目标作者使用了堆叠SiamRPN的思路可以在网络的前期抑制掉一些比较简单的样本块，在后期的网路中可以获得更加具有代表性的样本块；同时通过多级的回归操作可以使得输出的BB更加准确，会在一定程度上提升算法的精度，其实目标检测中也有类似的算法。</p><ul><li><img src="/2019/03/22/TRACKER/19.png" alt=""></li><li>论文：<a href="https://arxiv.org/abs/1812.06148" target="_blank" rel="noopener">https://arxiv.org/abs/1812.06148</a></li></ul></li><li><p>步骤</p><ul><li>步骤1：同时获取VID和YT-BB数据集作为训练集，并进行预处理操作；</li><li>步骤2：搭建如图所示的网络架构，首先训练第一阶段的SiamRPN，训练好之后接着训练下一个阶段的；</li><li>步骤3：使用特征转换模块将不同级的SiamRPN级联起来，使用最后的SiamRPN进行分类和回归，输出最终的结果。</li></ul></li><li><p>创新点</p><ul><li>步骤1：同时获取VID和YT-BB数据集作为训练集，并进行预处理操作；</li><li>步骤2：搭建如图所示的网络架构，首先训练第一阶段的SiamRPN，训练好之后接着训练下一个阶段的；</li><li>步骤3：使用特征转换模块将不同级的SiamRPN级联起来，使用最后的SiamRPN进行分类和回归，输出最终的结果。</li></ul></li></ul></li><li><p>SiamMask</p></li></ul><pre><code>- 思路是在saimese-fc的基础上添加了mask分支，和mask-rcnn有异曲同工之妙，mask分支的存在使用检测的结果更加准确，同时该算法可以获取跟踪目标的BB和Mask，这在现实应用中有很多的用处！！！    - 论文：https://arxiv.org/abs/1812.05050    - 项目：http://www.robots.ox.ac.uk/~qwang/SiamMask/    - 代码：http://www.robots.ox.ac.uk/~qwang/SiamMask/    - ![](TRACKER/20.png)- 步骤    - 步骤1：获取COCO、ImageNet-VID 和YouTube-VOS数据集作为训练数据集，并进行预处理；    - 步骤2：搭建如图所示的网络框架，关键在于RoW的引入和mask分支的实现，具体见论文，获得最终的模型；    - 步骤3：将预训练的模型应用在视频中，同时获取待跟踪目标的BB和Mask信息。- 创新点    - 创新1-使用一个网络同时解决了视频目标跟踪和视频目标分割问题；    - 创新2-通过mask分支来提升跟踪的效果；    - 创新3-引入RoW模获得准确的mask;    - 创新4-使用超大的数据集做训练，一般人是不敢用这么大的数据的，跑模型就需要很长时间，设备的差距！！！</code></pre><ul><li><p>CIR</p><ul><li><p>放在后面的都是用来压轴的，这是CVPR2019的一篇oral，仔细去看看你就会发现视觉目标跟踪近几年的oral论文小的可怜，而这篇论文能够成为oral肯定是解决了视觉目标跟踪中的一个大问题吧！没错，如果你是做跟踪的，你可能会发现我前面讲的这些算法的baseline网络都是AlexNet网络！！！很多人好奇干嘛不用ResNet、Inception等深度网络呢?其实跟踪领域中的很多学者们都尝试着用ResNet作为基准网络，但是却发现直接使用深层网络后后跟踪效果反而变差！！！这令人百思不得其解，因此这是孪生网络跟踪器中一个很重要的问题，结果这篇论文给出了我们一个详细的答案，最主要的原因是因为ResNet网络中都会有Padding操作，而这个操作会影响算法的平移不变性，使得网络更加关注图像中心</p><ul><li>论文：<a href="https://arxiv.org/abs/1901.01660" target="_blank" rel="noopener">https://arxiv.org/abs/1901.01660</a></li><li>代码：<a href="https://gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers" target="_blank" rel="noopener">https://gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers</a></li><li><img src="/2019/03/22/TRACKER/21.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：使用Siamese-fc或者SiamRPN的基本框架，我们仅仅将baseline网络更换成深层网络；</li><li>步骤2：针对ResNet中存在的pading问题加入了Crop层，思路即你既然会有影响我就把你去掉哈，思路及其简单去很有效果！</li><li>步骤3：作者设计了CIR和CIR-D块，然后通过堆叠这些块构建了深层网络，具体网络请看论文中；</li></ul></li><li><p>创新点</p><ul><li>创新1-深入的去探讨问题，而不是逃避问题，值得学习！！！</li><li>创新2-通过简单的Crop操作解决了一个关键的问题；</li><li>创新3-通过修改原始ResNet中的块构建出新的块；</li><li>创新4-论文中深刻的讨论了网络的输入大小、步长大小和padding操作的影响，并给出了合理的建议，这些建议有利于后续跟踪网络的自行设计！！！</li></ul></li></ul></li><li><p>SiamRPN++</p><ul><li><p>这也是CVPR2019中的一篇oral，而且是视觉目标跟踪的，所以看出跟踪在CVPR2019年有了很大的突破了，这是一个事实，后续会有更多优秀的论文投出，视觉目标跟踪距离真正的场景应用已经越来越近啦！！！这篇论文解决的第一个问题也是深层网络ResNet为何不能应用在孪生网络架构中提升性能！！！但是它提出了不同的方案，即作者发现原始的采样策略存在问题，原始的采样策略使得图像的中心一直有较大的权重，因此作者在中心进行移位，即偏移中心16-64个像素范围内进行均匀采样；而本文解决的另一个问题是相似目标的问题，对应的解决方案是抽取深度网络的多个特征层分别作分类和回归，并进行结果级联，思路和C-RPN很相似！！！</p><ul><li>论文：<a href="https://arxiv.org/abs/1812.11703" target="_blank" rel="noopener">https://arxiv.org/abs/1812.11703</a></li><li>项目：<a href="http://bo-li.info/SiamRPN++/" target="_blank" rel="noopener">http://bo-li.info/SiamRPN++/</a></li><li><img src="/2019/03/22/TRACKER/22.png" alt=""></li></ul></li><li><p>步骤</p><ul><li>步骤1：获取COCO 、ImageNet DET、 ImageNet VID和 YouTube-BoundingBoxes Dataset数据集来作为跟踪数据集，并进行预处理操作，数据集超级大！！！</li><li>步骤2：搭建上图所示的网络，去掉了ResNet中的两个降采样层，并分别对conv3、conv4和conv5的最后一层上面做分类和回归操做，网络的训练顺序也是依次训练；</li><li>步骤3：将预训练的网络应用到视频中，获得最终的分类和结果作为初步结果；</li><li>步骤4：进行后续的NMS等tricks后处理操作。</li></ul></li><li><p>创新点</p><ul><li>创新1-深刻的分析问题，理解问题的本质，并提出解决方案；</li><li>创新2-使用多级级联的思路获取鲁棒的特征表示；</li><li>创新3-在多个数据集中都是state-of-art结果；</li><li>创新4-使得基于孪生网络的跟踪器真正超越了基于相关滤波器的跟踪算法！！！</li></ul></li></ul></li><li><p>未来研究思路</p><ul><li>高效的在线学习算法：进展到目前为止，我的所有实验研究表明。Siamese 网络无法真正意义上抑制背景中的困难样本。离线的学习从本质上无法区分两个长相相似的人或者车。而 CF 相关算法可以通过分析整个环境的上下文关系来进行调整。如果对于提升整个算法的上界（偏学术）的角度考虑，在线学习有必要。如果正常的工程使用，我认为目前的算法只要在相应的场景中进行训练就足够了。</li><li>精确输出表达：今年我们的工作提出额外的 mask 输出。可直接扩展的思路为关键点输出（CornerNet / PoseTrack），极点预测（ExtremeNet），甚至 6D pose 跟踪。本质上是通过网络可以预测任何与目标相关的输出。大家可以任意的发散思维。</li><li>定制网络架构：其中包含两个子方向，一个是追求精度的去探索究竟什么样的网络架构会有利于当前的跟踪框架的学习。另一个有价值的子方向是如何构建超快速的小网络用于实际工程。工程项目中有时并没有 GPU 的资源供使用，如何提供 “廉价” 的高质量跟踪算法也具有很强的实际意义。当对网络进行裁剪之后，很容易达到 500FPS 的高性能算法来对传统的 KCF 进行真正的替换。</li><li>离线训练学习优化：目前的跟踪算法在相似性学习方向还是过于简单，如果去设计更为有效的度量学习方案，应该会有一定的提升。同时我们也并没有很好的掌握网络的训练。当前的训练策略是将网络主干的参数进行固定，先训练 head。然后逐步放开。实际上我们发现，当直接将所有层全部放开一起训练的时候，网络的泛化性能会显著下降。另一个方面，train from scratch 的概念已经在检测领域非常普遍了。跟踪的网络目前我们的经验在跟踪方面并不 work。</li><li>更细粒度预测：这一条实际上是上一条的续集，就是专注于 score 分支的预测。现在大家的做法是 &gt; 0.6 IoU 的都当做前景（正样本），但实际上正样本之间还是有较大的差异的。跟踪本质上也是不断预测一个非常细小物体帧间运动的过程，如果一个网络不能很好的分辨细小的差异，他可能并不是一个最优的设计选择。这也是 ATOM 的 IoUNet 主攻的方向。</li><li>泛化性能提升：非常推荐自动化所黄凯奇老师组的 GOT-10k 数据集，数据组织的非常棒。黄老师组在 one-shot learning 领域有着深厚的积淀，所以站在这个领域的角度，他们提出了严格分离训练集和测试集的物体类别来验证泛化性能。所以原则上所有 one-shot learning 方向的一些嵌入学习方法都可以移过来用。同时，我觉得 Mask-X-RCNN，segment everything 这个思路可以借鉴。本质上我也不得不承认，基于深度学习的跟踪算法存在泛化性能问题。我们有理由怀疑跟踪是否会在未知的类别上有较好的泛化性能，实际上肯定是会下降。</li><li>long-term 跟踪框架：截止到目前为止，虽然 VOT 组委会以及牛津这边的 OxUVA 都有专门的 long-term 的数据集，但 long-term 算法并没有一个较好的统一框架出来。关于这方面的研究似乎有点停滞，今年大连理工的文章非常可惜，我觉得质量非常不错。</li></ul></li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;目标跟踪方法调研&quot;&gt;&lt;a href=&quot;#目标跟踪方法调研&quot; class=&quot;headerlink&quot; title=&quot;目标跟踪方法调研&quot;&gt;&lt;/a&gt;目标跟踪方法调研&lt;/h1&gt;&lt;h2 id=&quot;经典算法&quot;&gt;&lt;a href=&quot;#经典算法&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="TRACKER" scheme="http://yoursite.com/tags/TRACKER/"/>
    
  </entry>
  
  <entry>
    <title>TLD</title>
    <link href="http://yoursite.com/2019/03/22/TLD/"/>
    <id>http://yoursite.com/2019/03/22/TLD/</id>
    <published>2019-03-22T02:30:20.000Z</published>
    <updated>2019-03-22T03:30:18.364Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TLD"><a href="#TLD" class="headerlink" title="TLD"></a>TLD</h2><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><ul><li>将传统的跟踪算法和传统的检测算法相结合来解决被跟踪目标在被跟踪过程中发生的形变、部分遮挡等问题。</li><li>通过一种改进的在线学习机制不断更新跟踪模块的“显著特征点”和检测模块的目标模型及相关参数，从而使得跟踪效果更加稳定、鲁棒、可靠。</li></ul><h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><ul><li><p>长时间跟踪的关键问题</p><ul><li>目标重新出现在相机视野中， 系统能检测到它，并开始重新跟踪</li><li><p>难点</p><ul><li>过程中目标发生形状变化， 光照，尺度， 遮挡</li></ul></li></ul></li></ul><h3 id="原有方法"><a href="#原有方法" class="headerlink" title="原有方法"></a>原有方法</h3><ul><li>原有跟踪方法， 当检测模块检测到被跟踪目标后， 开始进入跟踪模块</li><li><p>步骤</p><ul><li>检测模块检测目标</li><li>当检测到被跟踪目标后， 进入跟踪模块</li><li>此后检测模块不会介入到跟踪过程</li><li><p>缺陷</p><ul><li>被跟踪目标发生形变遮挡时， 跟踪容易失败</li></ul></li></ul></li></ul><h3 id="我的理解思路"><a href="#我的理解思路" class="headerlink" title="我的理解思路"></a>我的理解思路</h3><ul><li>所以需要找到一种方法可以识别出目标的形变（目标尽管发生形变，但依然存在的相似性）</li><li>如果要训练各种形变的情况， 样本量是非常小的， 而且对于从来没有出现过的目标， 根本就没有训练样本</li></ul><h3 id="TLD主要框架"><a href="#TLD主要框架" class="headerlink" title="TLD主要框架"></a>TLD主要框架</h3><ul><li><p>tracking</p><ul><li>利用当前帧和下一帧</li><li><p>作用</p><ul><li>跟踪连续帧间的运动， 当物体始终可见时跟踪器才会有效</li></ul></li><li><p>预测下一帧目标</p><ul><li>利用金字塔光流法，和norm cross correlation预测</li><li>根据物体在前一帧已知的位置估计在当前帧的位置， 这样就会产生仪表物体运动的轨迹， 从这条轨迹可以为学习模块产生正样本</li></ul></li><li><p>步骤</p><ul><li>1、先从当前帧目标均匀取10*10个点，记为n1。</li><li>2、利用这些点用光流法预测取下一帧对应的点n2</li><li>3、再用这些对应的点用光流法返回预测当前帧所在的点n3，n1和n3的距离就是误差（FB_error）</li><li>4、ncc利用n1的每个点和n2每个点为中心提取10*10的像素矩阵</li><li>5、对上一步中得到的像素矩阵， 使用亚像素精度， 两者进行计算互相关归一化模板匹配， 得到相似值</li><li>6、取大于ncc相似值中值和小于FB_error的n2点和对应的n1点， 基于这些点计算原目标缩放和位移</li></ul></li><li><p>追踪算法</p><ul><li><p>Median-Flow追踪算法</p><ul><li>采用的是Lucas-Kanade追踪器，也就是常说的光流法追踪器</li><li>只需要知道给定若干追踪点，追踪器会根据像素的运动情况确定这些追踪点在下一帧的位置。</li></ul></li><li><p>TLD原作认为一个好的追踪算法</p><ul><li><p>具有正反向连续性</p><ul><li>无论是按照时间上的正序追踪还是反序追踪， 产生的轨迹应该是一样的。 </li><li><p>根据这个性质规定了FB误差FB_error</p><ul><li>从时间t的初始位置x(t)开始追踪产生时间t+p的位置x(t+p)，再从位置x(t+p)反向追踪产生时间t的预测位置x`(t)，初始位置和预测位置之间的欧氏距离就作为追踪器在t时间的FB误差。</li><li><img src="/2019/03/22/TLD/1.png" alt=""></li></ul></li></ul></li></ul></li><li><p>追踪点的选择</p><ul><li>首先在上一帧t的物体包围框里均匀地产生一些点，然后用Lucas-Kanade追踪器正向追踪这些点到t+1帧，再反向追踪到t帧，计算FB误差，筛选出FB误差最小的一半点作为最佳追踪点。</li><li>最后根据这些点的坐标变化和距离的变化计算t+1帧包围框的位置和大小（平移的尺度取中值，缩放的尺度取中值。</li></ul></li></ul></li></ul></li><li><p>learning</p><ul><li>利用第一帧，和框定目标，训练，初始化在线模型</li><li>用预测到的下一帧目标， 来学习修正Randofern在线模型， 和最近邻在线模型</li><li>根据追踪器和检测器产生的正负样本， 迭代训练分类器， 改善检测器的精度</li><li><p>P-N学习</p><ul><li>半监督机器学习算法</li><li>针对检测器对样本分类时产生的两种错误， 提供两种“专家”进行纠正</li><li><p>两个专家</p><ul><li><p>P专家</p><ul><li>检出漏检的正样本（正样本被分为负样本）</li></ul></li><li><p>N专家</p><ul><li>改正误检的正样本（负样本误分为正样本）</li></ul></li></ul></li><li><p>样本产生</p><ul><li>使用扫描框对图像逐行扫描，确定的区域成为图像元patch, 成为一个样本</li><li>扫描产生的样本是未标签样本， 需要用分类器分类，确定它的标签</li><li><p>如果算法言确定物体在t+1帧的位置</p><ul><li><p>从检测器产生的包围框中， 筛选出十个与物体位置距离最近的包围框</p><ul><li>两个包围框的交面积/并面积&gt;0.7</li></ul></li><li><p>对每个包围框做微小的仿射变化， 产生20个图像元，这样就产生了200个正样本</p></li><li><p>在选出若干记录较远的包围框，产生负样本</p><ul><li>交的面积/并的面积&lt;0.2</li></ul></li></ul></li><li><p>这样产生的样本是已标签的样本， 把这些样本放入训练集， 用于更新分类器的参数</p></li><li><img src="/2019/03/22/TLD/2.png" alt=""></li></ul></li><li><p>算法的结果应该具有“结构性”</p><ul><li>每一针图像内，物体最多只出现在一个位置</li><li>相邻帧物体间的运动是连续的， 连续帧的位置可以构成一条较平滑的轨迹</li><li>整个追踪过程中， 轨迹可能是分段的， 因为物体有可能中途消失， 之后再度出现</li><li><p>P专家的作用</p><ul><li>寻找数据在<strong>时间</strong>上的结构性</li><li>利用<strong>追踪器</strong>的结果预测物体在t+1帧的位置</li><li>如果这个位置（包围框）被检测器分类为负，P专家就把这个位置改为正。</li><li>P专家要保证物体在连续帧上出现的位置可以构成连续的轨迹</li></ul></li><li><p>N专家的作用</p><ul><li>寻找数据在<strong>空间</strong>上的结构性</li><li>把<strong>检测器</strong>产生的和P专家产生的所有正样本进行比较，</li><li>选择出一个最可信的位置，保证物体最多只出现在一个位置上，</li><li>把这个位置作为TLD算法的追踪结果。同时这个位置也用来重新初始化追踪器。</li></ul></li></ul></li></ul></li></ul></li><li><p>detection</p><ul><li><p>通过learing学习到的在线模型进行目标预测</p><ul><li>对每一帧图像都做全面的扫描</li><li>找到与目标物体相似的所有外观的位置</li><li>从检测产生的结果中，产生正样本， 和负样本， 交给学习模块</li><li>从所有正样本和负样本中选出一个最可信的位置，作为这一帧TLD的输出结果</li><li>然后用这个结果更新追踪器的起始位置</li></ul></li><li><p>作用</p><ul><li>估计跟踪器的误差， 如果误差很大就改正追踪器的结果 </li></ul></li><li><p>根据目标大小，把一帧图像分割为很多不同尺寸的图像片</p></li><li><p>三层级联分类器：把不适合的图像片去掉，剩余的就是可能是目标的图像片</p><ul><li><p>方差分类器</p><ul><li>若图像片的方差小于【方差在线模型】的一半， 则认为这些是背景，丢弃。</li></ul></li><li><p>RandomFern分类器</p><ul><li><p>利用【RandomFern在线模型】计算每个图像片的后验概率</p><ul><li>训练时， 把图像片与在线模型重叠度＞0.6的分为 好的box，＜0.2为坏的box。</li><li>每个图像片会产生13个点对，利用这些点对产生13位二进制码，上二进制码记为1</li><li>最后，用好的box产生的每个二进制码的出现次数/（好box+坏box的出现次数），便得出每个不同二进制码的概率</li></ul></li><li><p>若小于阈值，就可以丢弃</p></li></ul></li><li><p>最近邻分类器</p><ul><li>利用【最近邻在线模型】计算出图像片与在线模型的相关相似度</li><li>若相关相似度大于阈值， 才能通过分类器， 否则被丢弃</li></ul></li></ul></li><li><p>其他参考文章</p><ul><li><a href="https://blog.csdn.net/sinat_31135199/article/details/70739106" target="_blank" rel="noopener">https://blog.csdn.net/sinat_31135199/article/details/70739106</a></li></ul></li></ul></li><li><p>根据跟踪器和检测器预测下一帧目标所在位置</p><ul><li><p>1、跟踪器：跟踪到目标图像片t1</p><ul><li>如果跟踪不到目标， 但检测目标聚类只有一种， 就用该聚类结果预测目标位置</li></ul></li><li><p>2、检测器：检测到目标</p></li><li><p>3、检测器：将检测到的图像片聚类</p><ul><li>若只有一个结果与检测结果差距较远， 且检测器的conf较大， 用检测器结果纠正跟踪器</li><li>若能检测不能跟踪，就用检测结果纠正跟踪器</li></ul></li><li><p>4、检测器：找出聚类后、与跟踪器找到的图像片，重叠度小于0.5,且相似相关度大的图像片， d1</p></li><li>利用t1和d1进行加权平均</li><li>预测到下一帧目标所在位置</li></ul></li></ul><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ul><li><img src="/2019/03/22/TLD/3.png" alt=""></li><li>1、检测器由一系列包围框产生样本， 经过三层级联分类器产生正样本， 放入样本集。</li><li>2、追踪器：估计出物体的新位置</li><li>3、P专家：根据追踪器估计的新位置产生正样本</li><li>4、N专家：从这些正样本里选出一个最可信的，同时将其他正样本标记为负</li><li>5、最后用正样本更新检测器的分类器参数， 并确定下一帧物体包围框的位置</li></ul><h3 id="程序运行框架"><a href="#程序运行框架" class="headerlink" title="程序运行框架"></a>程序运行框架</h3><ul><li><img src="/2019/03/22/TLD/4.png" alt=""></li><li><img src="/2019/03/22/TLD/5.png" alt=""></li><li><img src="/2019/03/22/TLD/6.png" alt=""></li><li><img src="/2019/03/22/TLD/7.png" alt=""></li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;TLD&quot;&gt;&lt;a href=&quot;#TLD&quot; class=&quot;headerlink&quot; title=&quot;TLD&quot;&gt;&lt;/a&gt;TLD&lt;/h2&gt;&lt;h3 id=&quot;是什么&quot;&gt;&lt;a href=&quot;#是什么&quot; class=&quot;headerlink&quot; title=&quot;是什么&quot;&gt;&lt;/a&gt;是什么&lt;/h
      
    
    </summary>
    
    
      <category term="Tracking" scheme="http://yoursite.com/tags/TRACKING/"/>
    
  </entry>
  
  <entry>
    <title>Thinking note of PNN</title>
    <link href="http://yoursite.com/2019/03/17/THINKING-NOTE-OF-PNN/"/>
    <id>http://yoursite.com/2019/03/17/THINKING-NOTE-OF-PNN/</id>
    <published>2019-03-17T02:32:52.000Z</published>
    <updated>2019-03-18T01:34:19.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Thinking-note-of-PNN"><a href="#Thinking-note-of-PNN" class="headerlink" title="Thinking note of PNN"></a>Thinking note of PNN</h1><h1 id="汇报PPT"><a href="#汇报PPT" class="headerlink" title="汇报PPT"></a>汇报PPT</h1><h2 id="PNN"><a href="#PNN" class="headerlink" title="PNN"></a>PNN</h2><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><ul><li><p>Plastic Network塑性网络</p><ul><li><p>Plastic可塑性， 灵感来源</p><ul><li><p>来自神经生物学中的 突触可塑性</p><ul><li>是指神经细胞之间的连接， 即突触，其连接强度可调节的特性。</li><li>比如，巴甫洛夫的经典条件反射实验，每次给狗喂食的同时摇铃。当狗见到食物时，自然会分泌唾液。但巴甫洛夫发现，狗在听到铃声时即使不给食物，也会分泌唾液。</li></ul></li><li><p>赫布理论</p><ul><li>突触前神经元向突触后神经元的持续重复的刺激可以导致突触传递效能的增加</li><li>反射活动的持续与重复会导致神经元稳定性的持久性提升</li><li>当神经元A的轴突与神经元B很近并参与了对B的重复持续的兴奋时，这两个神经元或其中一个便会发生某些生长过程或代谢变化，致使A作为能使B兴奋的细胞之一，它的效能增强了。</li><li><p>神经元”A”必须对神经元”B”的激发“作出了一定贡献”，神经元”A”的激发必须在神经元”B”之先，而不能同时激发。</p><ul><li>tips: 神经元之间的神经冲动是单方向传导， 只能由一个神经元的轴突传导给另一个神经元的细胞体或树突 ，而不能向相反的方向传导，因为神经递质只能在突触前神经元的轴突末梢释放。 </li></ul></li></ul></li><li><p>突触</p><ul><li>根据突触接触部位分类</li><li><p>神经细胞之间如何开始建立突触的</p><ul><li>上级神经细胞的轴突末端的生长锥顺着信号分子的指引来到目的神经细胞，然后通过来回的物质传递交换，来形成进一步更紧密的关系。</li><li>上级神经细胞的生长锥会特化为轴突末端，形成突触前膜，准备释放神经递质。突触后细胞会在突触结合的位置特化出突触后膜，聚集了神经递质的受体和电压门控钙通道。</li><li>两个细胞形成稳定联系了，就会把一些多余的突触结构简化掉，只留下足够有效的突触数量。</li></ul></li></ul></li><li><p>本文使用的规则</p><ul><li><p>赫布规则：if a neuron repeatedly takes part in making another neuron fire, the connection between them is strengthened </p><ul><li><blockquote><p>如果一个神经元不断参与另一个神经元的激活，它们之间的联系就会加强</p></blockquote></li></ul></li></ul></li></ul></li><li><p>人工神经网络中的非塑性网络</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/4.png" alt=""></li><li>前后神经元之间的影响通过一个权值参数来表示</li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/5.png" alt=""></li><li>使用梯度下降方法训练网络使得权值收敛</li><li>在训练完成后，权值参数不再变化</li></ul></li><li><p>人工神经网络中的塑性网络</p><ul><li><p>前后神经元之间的影响会随着前级神经元对后级神经元持续重复的刺激增加。</p><ul><li>如果两个神经元之间的联系减弱，则影响会降低</li><li>如果两个神经元之间的联系增强，则影响会增强</li></ul></li><li><p>这样的影响是如何表示的呢？</p><ul><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/1.png" alt=""></p><ul><li>(@？如何从神经细胞学的角度来解释这个公式呢？)</li></ul></li><li><p>基础权重</p><ul><li>@？初始值</li></ul></li><li><p>非线性函数$\sigma$</p><ul><li>@？在公式中代表了什么？</li></ul></li><li><p>赫布迹</p><ul><li>存储着塑性组件</li><li>@代表前级神经元参与到后级神经元的神经活动中，联系的程度</li></ul></li><li><p>塑性系数</p><ul><li>表示了塑性组件Hebbian迹和基础权重的相对重要性</li></ul></li><li><p>全局塑性系数</p><ul><li>塑性系数与赫布迹相乘，得到全局的塑性组件</li></ul></li><li><p>学习率$\elta$</p><ul><li>是整个网络的学习标量参数</li><li>作为权重衰减项出现， 保证了Hebbian迹的失控正反馈</li></ul></li></ul></li></ul></li></ul></li><li><p>在小样本学习中的关系</p></li></ul><h3 id="可以解决什么问题"><a href="#可以解决什么问题" class="headerlink" title="可以解决什么问题"></a>可以解决什么问题</h3><ul><li>记忆和重建训练中没有看到的新颖的高维自然图像集，传统非塑性神经网络无法解决这个问题</li><li>可以为非琐碎的任务（ non-trivial tasks ）训练大型网络</li></ul><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul><li><p>实验1，模式记忆之二元模式实验</p><ul><li><p>二元模式</p><ul><li>一些长度为N向量，向量的每个元素只有1和-1两种取值。</li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/6.png" alt="概念序列"></li><li>每个二元模式由1000个元素组成， 每一个不是1就是-1， 分别用红色和蓝色表示。 </li></ul></li><li><p>网络结构</p><ul><li><p>使用Hebbian plastic 连接的人工循环神经网络</p><ul><li>@？循环神经网络是什么</li></ul></li><li><p>可以执行这个任务的神经网络被称为内容可寻址记忆， 或者 自动联想神经网络</p></li><li>可微的塑性网络可以自动解决这个问题，它可以 自动设计可以执行现有的人工神经网络可以完成的任务 的神经网络</li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/7.png" alt="完全循环神经网络"></li><li><p>每一个模式元素都有一个神经元。 加上一个固定的输出神经元（“bias”），一共有（1001）个神经元</p><ul><li>@？输出神经元是如何与其他神经元连接的？</li></ul></li><li><p>在输入值不为0的情况下，输入的模式以模式中对应元素的值来固定每个神经元的值</p></li><li><p>如果是降维模式下的输入，对于该模式下的0值，对应的神经元不会接收输入， 只从横向连接（lateral connections）获得输入，它们必须从横向连接重建正确的预期输出值。</p><ul><li>@？横向连接是什么？</li></ul></li><li><p>输出直接从神经元s的激活（activation）中读取</p><ul><li>@?激活？</li></ul></li><li><p>使用了简单的衰减的赫布规则来更新赫布迹</p></li><li>这个神经网络有两个训练参数 ，1001个神经元， 加起来就是 $1001<em>1001</em>2$个参数需要优化</li></ul></li><li><p>训练过程</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/8.png" alt=""></li><li><p>输入训练样本</p><ul><li>一组二元模式序列</li><li>循环重复地输入若干个不同的二元模式</li></ul></li><li><p>降维退化模式</p><ul><li>从输入的训练样本中随机选择一个作为预测模式，通过将该模式的一半位设置为0来对其进行降级</li><li>将降级后的预测模式输入给网络</li><li>网络会对预测模式重建，在网络的记忆中绘制出损失的位（图中用浅色表示预测），得到一个针对预测模式的预测输出值</li><li>利用该预测输出值，与模式正确值之间的平方和误差（the summed squared error）作为损失（loss）</li><li><p>根据这个损失进行梯度下降，反向传播， 更新两类权值参数： 基本权值$w_{i,j}$与塑性系数$\alpha_{i,j}$</p><ul><li>这些系数是通过一个Adam求解器使用学习率0.001优化的</li><li>@？如何进行梯度下降，反向传播，更新两类权值参数的？</li></ul></li></ul></li></ul></li><li><p>效果如何</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/9.png" alt=""></li><li>上图给出了10次使用不同随机种子训练的结果</li><li>在大概200个周期是，错误率收敛到很低，残留的错误小于1%</li><li>与之相对比的，使用非塑性循环神经网络的学习曲线如下</li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/10.png" alt=""></li><li>使用了50位的二元模式输入</li><li>绿色表示， 2050个神经元的非塑性循环神经网络的效果</li><li>红色表示，2050个神经元的LSTM的效果</li><li>蓝色表示，使用相同参数下，只使用了51个神经元，可微带塑性权重的神经元的效果</li><li><p>塑性网络和非塑性循环神经网络的比较</p><ul><li>非塑性网络也可以解决这个任务，但是需要额外的神经元存储事先已经见过的模式</li><li>然而，尽管有了很多探索，我们都不能成功的使用非塑性RNN或LSTM解决这个任务</li><li>我们只能减少模式规模到50位，每个生命周期只有2个模式， 并且只展示3个生命周期步骤</li><li>最好的结果也需要增加2000个额外的神经元（总共2050个神经元）</li><li>对于非塑性RNN， 误差稳定在一个比较高的地方</li><li>对于LSTM，解决了这个任务，但是用了大概500000个周期</li><li>而塑性循环神经网络， 在2000个生命周期里迅速使得误差率降到了0.01，比LSTM快250倍。</li></ul></li><li><p>启发</p><ul><li>尽管这个任务在PNN中表现比LSTM好</li><li>那么在其他的领域中，也可能比现有的LSTM模型表现更好</li><li>甚至可以通过给现有的LSTM模型增加可塑性</li></ul></li></ul></li></ul></li><li><p>实验2，模式记忆之自然图像</p><ul><li><p>自然图像</p><ul><li><p>输入的数据集来自CIFAR-10， 是32*32的灰度图像</p><ul><li>包含了6000张</li></ul></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/11.png" alt=""></p></li></ul></li><li><p>网络结构与训练过程</p><ul><li><p>与实验1二元模式基本保持一致</p><ul><li>有1025个神经元</li><li>$2<em>1025</em>1025$个参数</li><li>每个生命周期包含3个图片，每次随机显示3次，一共20个时间步长，在图像展示中有三个输入为0的时间步骤</li></ul></li><li><p>不同在于，在降维退化的时候， 丢失了图像的上半部分或者下半部分</p><ul><li>这样避免了相邻像素之间天然的关联性</li><li>@？那么在退化以后输入给神经网络，这时，对于空白的神经元，这些神经元的值是从哪里来的呢？</li></ul></li></ul></li><li><p>结果</p><ul><li><p>模型成功学习到去表现这个记忆重建原来未见到的自然图像的non-trial任务</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/11.png" alt=""></li></ul></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/12.png" alt="基线权重矩阵">训练最终的权值矩阵</p></li><li>每一列描述了单个细胞的输入，垂直相邻条目描述 来自图像中水平相邻像素的输入。 </li><li>请注意两个矩阵之间的重要结构。</li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/13.png" alt="塑性系数矩阵">训练最终的塑性系数矩阵</p><ul><li>与传统同质塑性Hopfield网络相比， 这个塑性矩阵显示了相当大的结构</li><li>一些结构(对角线)与相邻像素的高相关性有关，</li><li>而另一些方面(中段附近的交替带)则是由于选择在预测图像中对一半的区域归零导致的</li><li>我们假设，当一个测试刺激出现时，这个中段附近的宽交替带支持快速清理持续活动的网络。</li></ul></li><li><p>虽然所学习的网络结构是有结构的，这种结构可能只是学习过程的产物，没有内在的有用性。</p><ul><li>为了检验这种可能性，我们比较了全塑性网络和一个具有共享塑性系数的相似网络， （所有连接共享相同的塑性系数$\alpha$）</li><li>因此塑性系数仍然可训练，但是作为一个单独的参数在所有连接中共享。</li><li><p>因为我们用了衰减的赫布公式， 这个共享塑性结构与一个快速权重网络具有相似性。 </p><ul><li>然而，与快速权值方法不同，可微塑性允许我们通过梯度下降学习快速权值的重要性及其学习率(而且，我们不实现快速权值网络使用的每个时间步长的神经活动迭代计算)。</li><li>@？这个快速权重网络是什么？</li></ul></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/14.png" alt="二者比较">塑性系数矩阵</p><ul><li>主要结果是，每个连接的独立塑性系数提高了这项任务的性能。</li><li>这种比较表明，塑性系数矩阵和权重矩阵图中所观察到的结构实际上是有用的，在这样的设置下，为记忆和重建自然图像构建了一个创新的结构</li></ul></li></ul></li></ul></li></ul></li><li><p>实验3、一元模式分类</p></li></ul><h3 id="具体是如何解决的"><a href="#具体是如何解决的" class="headerlink" title="具体是如何解决的"></a>具体是如何解决的</h3><ul><li>网络是如何设计的？</li><li>网络是如何训练的？</li><li><p>基础权值是如何优化的？</p><ul><li>在整个生命周期被保存</li><li>通过梯度下降法优化</li><li>在一个生命周期最大化期待的性能</li></ul></li><li><p>塑性系数是如何优化的？</p><ul><li>在整个生命周期被保存</li><li>通过梯度下降法优化</li><li>在一个生命周期最大化期待的性能</li></ul></li><li><p>Hebbian赫布迹是如何更新的？</p><ul><li>按照一定规则优化，有很多规则，下面介绍两种规则</li><li><p>会衰减的赫布迹更新规则</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/2.png" alt=""></li><li><p>迭代更新步骤</p><ul><li>Hebbian迹在每个生命周期的开始初始化为0 </li><li>@？分析公式如何更新</li><li>@？学习率$\elta$从何而来，如何更新？</li></ul></li><li><p>缺点</p><ul><li>因为权重衰减， hebb迹在输入缺少的情况下衰减到0</li></ul></li></ul></li><li><p>Oja’s 规则</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/3.png" alt=""></li><li><p>优势</p><ul><li>可以在刺激缺少的情况下，无限保持权重值的稳定性。</li><li>可以训练神经网络形成持续任意时间的记忆</li></ul></li><li><p>迭代更新步骤</p><ul><li>@？分析公式如何更新</li></ul></li></ul></li></ul></li><li><p>为什么这样就能解决呢？</p></li></ul><h3 id="解决的效果怎样"><a href="#解决的效果怎样" class="headerlink" title="解决的效果怎样"></a>解决的效果怎样</h3><h3 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h3><ul><li>为基于梯度的神经网络训练开辟了一条新的研究途径，而且具有重要的现实意义</li><li>同时也证明了神经结构的元特性通常与进化或先天设计有关，实际上是可以梯度下降的</li></ul><h2 id="先导知识"><a href="#先导知识" class="headerlink" title="先导知识"></a>先导知识</h2><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/17.png" alt=""></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/18.png" alt=""></p><ul><li>@？神经元的数目与隐藏层向量的维度相同</li><li>$s_t$表示在t时刻向量s的值</li></ul></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/19.png" alt="">  循环神经网络的计算方法</p></li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/20.png" alt=""></li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/21.png" alt="双向卷积神经网络"></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/22.png" alt="双向卷积神经网络">双向卷积神经网络的计算方法</p><ul><li>正向计算和反向计算不共享权重，也就是说U和U’、W和W’、V和V’都是不同的权重矩阵。</li></ul></li><li><p><img src="/2019/03/17/THINKING-NOTE-OF-PNN/23.png" alt="深度循环神经网络"> 深度循环神经网络</p><ul><li>堆叠两个以上的隐藏层</li></ul></li><li><p>循环神经网络的训练算法：BPTT</p><ul><li><p>三个步骤</p><ul><li><p>前向计算每个神经元的输出值；</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/24.png" alt=""></li></ul></li><li><p>反向计算每个神经元的误差项$\deta_j$， 是误差函数E对神经元j的加权输入的偏导数。 </p><ul><li><p>误差项</p><ul><li>第l层t时刻的误差项$\deta_t^l$的值言两个方向传播</li><li>一个方向是传递到上一层网络，得到$\deta_t^{l-1}$，这部分之和权重矩阵U有关。 </li><li>另一个方向是沿时间线传递到初始$t_1$时刻， 这部分只和权重矩阵W有关</li></ul></li><li><p>神经元在t时刻的加权输入</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/25.png" alt=""></li></ul></li><li><p>误差函数对神经元j的加权输入的偏导数</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/26.png" alt=""></li><li>就是将误差项沿时间反向传播的算法</li></ul></li><li><p>将误差项传递到上一层算法</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/27.png" alt=""></li></ul></li></ul></li><li><p>计算每个权重的梯度</p><ul><li><p>梯度来自</p><ul><li>任意一个时刻的误差项</li><li>上一个时刻循环层的输出值</li></ul></li></ul></li><li><p>最后用随机梯度下降算法更新权重</p></li></ul></li></ul></li></ul><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><ul><li>从输出结果开始，求上一层的误差</li><li>求得所有误差后，使用误差来更新权值</li></ul><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><h3 id="权值更新"><a href="#权值更新" class="headerlink" title="权值更新"></a>权值更新</h3><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul><li><p>感知器（使用阶跃函数作为激活函数的神经元）</p><ul><li><p>输入</p><ul><li><p>输入权值</p><ul><li>每个输入上有一个权值</li><li>此外还有一个bias 就是权值w0*输入1</li></ul></li></ul></li><li><p>激活函数</p></li><li><p>输出</p><ul><li>就是权值与对应输入相乘（含w0和x0=1）</li><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/16.png" alt=""></li></ul></li></ul></li><li><p>感知器训练算法</p><ul><li>将权重初始化=0</li><li>利用感知器规则迭代修改w</li><li><p>感知器规则</p><ul><li><img src="/2019/03/17/THINKING-NOTE-OF-PNN/15.png" alt=""></li></ul></li></ul></li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Thinking-note-of-PNN&quot;&gt;&lt;a href=&quot;#Thinking-note-of-PNN&quot; class=&quot;headerlink&quot; title=&quot;Thinking note of PNN&quot;&gt;&lt;/a&gt;Thinking note of PNN&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Plasticity Neural Networks</title>
    <link href="http://yoursite.com/2019/03/14/PLASTICITY-NEURAL-NETWORKS/"/>
    <id>http://yoursite.com/2019/03/14/PLASTICITY-NEURAL-NETWORKS/</id>
    <published>2019-03-14T12:48:55.000Z</published>
    <updated>2019-03-17T00:46:44.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Plastic-NN"><a href="#Plastic-NN" class="headerlink" title="Plastic NN"></a>Plastic NN</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="How-can-we-build-agents-that-keep-learning-from-experience-after-their-initial-training"><a href="#How-can-we-build-agents-that-keep-learning-from-experience-after-their-initial-training" class="headerlink" title="How can we build  agents that keep learning from experience , after their initial training ?"></a>How can we build  agents that keep learning from experience , after their initial training ?</h3><h3 id="Take-inspiration-from-the-main-mechanism-of-learning-in-biological-brains-synaptic-plasticity"><a href="#Take-inspiration-from-the-main-mechanism-of-learning-in-biological-brains-synaptic-plasticity" class="headerlink" title="Take inspiration from the main mechanism of learning in biological brains : synaptic plasticity"></a>Take inspiration from the main mechanism of learning in biological brains : synaptic plasticity</h3><h3 id="Synaptic-plasticity-carefully-tuned-by-evolution-to-produce-efficient-lifelong-learning"><a href="#Synaptic-plasticity-carefully-tuned-by-evolution-to-produce-efficient-lifelong-learning" class="headerlink" title="Synaptic plasticity , carefully tuned by evolution to produce efficient lifelong learning ."></a>Synaptic plasticity , carefully tuned by evolution to produce efficient lifelong learning .</h3><h3 id="we-show-that-plasticity-just-like-connection-weights-can-be-optimized-by-gradient-descent-in-large-recurrent-networks-with-hebbian-plastic-connections"><a href="#we-show-that-plasticity-just-like-connection-weights-can-be-optimized-by-gradient-descent-in-large-recurrent-networks-with-hebbian-plastic-connections" class="headerlink" title="we show that plasticity , just like connection weights , can be optimized by gradient descent in large recurrent networks with hebbian plastic connections ."></a>we show that plasticity , just like connection weights , can be optimized by gradient descent in large recurrent networks with hebbian plastic connections .</h3><h3 id="First-recurrent-plastic-networks-with-more-than-two-million-parameters-can-be-trained-to-memorize-and-reconstruct-sets-of-novel-high-dimensional-natural-images-not-seen-during-training"><a href="#First-recurrent-plastic-networks-with-more-than-two-million-parameters-can-be-trained-to-memorize-and-reconstruct-sets-of-novel-high-dimensional-natural-images-not-seen-during-training" class="headerlink" title="First , recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel , high-dimensional natural images not seen during training ."></a>First , recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel , high-dimensional natural images not seen during training .</h3><ul><li>超过两百万参数的递归塑性网络可以被训练来记忆和重建，在训练中没有出现过的创新高维的自然图像数据集。 </li></ul><h3 id="Crucially-traditional-non-plastic-recurrent-networks-fail-to-solve-this-task"><a href="#Crucially-traditional-non-plastic-recurrent-networks-fail-to-solve-this-task" class="headerlink" title="Crucially , traditional non-plastic recurrent networks fail to solve this task ."></a>Crucially , traditional non-plastic recurrent networks fail to solve this task .</h3><ul><li>至关重要的是，传统非塑性循环神经网络无法解决这个问题</li></ul><h3 id="Furthermore-trained-plastic-networks-can-also-solve-generic-meta-learning-tasks-such-as-the-Omniglot-task，with-competitive-results-and-little-parameter-overhead"><a href="#Furthermore-trained-plastic-networks-can-also-solve-generic-meta-learning-tasks-such-as-the-Omniglot-task，with-competitive-results-and-little-parameter-overhead" class="headerlink" title="Furthermore , trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task，with competitive results, and little parameter overhead."></a>Furthermore , trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task，with competitive results, and little parameter overhead.</h3><ul><li>更好的是， 训练塑性神经网络也可以解决像“Omniglot”任务， 有着复杂结果和无限参数的通用元学习任务</li></ul><h3 id="Finally-in-reinforcement-learning-settings-plastic-networks-outperform-a-non-plastic-equivalent-in-a-maze-exploration-task"><a href="#Finally-in-reinforcement-learning-settings-plastic-networks-outperform-a-non-plastic-equivalent-in-a-maze-exploration-task" class="headerlink" title="Finally , in reinforcement learning settings , plastic networks outperform a non-plastic equivalent in a maze exploration task."></a>Finally , in reinforcement learning settings , plastic networks outperform a non-plastic equivalent in a maze exploration task.</h3><ul><li>最后， 在强化学习迷宫问题中，塑性网络比非塑性网络的表现效果更好</li></ul><h3 id="We-conclude-that-differentiable-plasticity-may-provide-a-powerful-novel-approach-to-the-learning-to-learn-problem"><a href="#We-conclude-that-differentiable-plasticity-may-provide-a-powerful-novel-approach-to-the-learning-to-learn-problem" class="headerlink" title="We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem ."></a>We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem .</h3><ul><li>总结，可微塑性网络也许为“learn to learn ”问题提供了更有力创新的方法。</li></ul><h2 id="Introduction-the-problem-of-“learn-to-learn-“"><a href="#Introduction-the-problem-of-“learn-to-learn-“" class="headerlink" title="Introduction : the problem of “learn to learn “"></a>Introduction : the problem of “learn to learn “</h2><h3 id="Many-of-the-recent-spectacular-successes-in-machine-learning-involve-learning-one-complex-task-very-well-Through-extensive-training-over-thousands-or-millions-of-training-examples"><a href="#Many-of-the-recent-spectacular-successes-in-machine-learning-involve-learning-one-complex-task-very-well-Through-extensive-training-over-thousands-or-millions-of-training-examples" class="headerlink" title="Many of the recent spectacular successes in machine learning involve learning one complex task very well,Through extensive training over thousands or millions of training examples ."></a>Many of the recent spectacular successes in machine learning involve learning one complex task very well,Through extensive training over thousands or millions of training examples .</h3><h3 id="After-learning-is-complete-the-agent’s-knowledge-is-fixed-and-unchanging"><a href="#After-learning-is-complete-the-agent’s-knowledge-is-fixed-and-unchanging" class="headerlink" title="After learning is complete, the agent’s knowledge is fixed and unchanging ;"></a>After learning is complete, the agent’s knowledge is fixed and unchanging ;</h3><ul><li>现在的学习方法中， 智能体的知识在学习完后是固定不变的</li></ul><h3 id="If-the-agent-is-to-be-applie-to-a-different-task-it-must-be-re-trained-again-requiring-a-very-large-number-of-new-training-examples"><a href="#If-the-agent-is-to-be-applie-to-a-different-task-it-must-be-re-trained-again-requiring-a-very-large-number-of-new-training-examples" class="headerlink" title="If the agent is to be applie to a different task , it must be re-trained , again requiring a very large number of new training examples."></a>If the agent is to be applie to a different task , it must be re-trained , again requiring a very large number of new training examples.</h3><ul><li>如果智能体被要求执行一个不同的任务，它必须被重新训练， 在此需要大量的训练数据。</li></ul><h3 id="By-constrast-biological-agents-exhibit-a-remark-able-ability-to-learn-quickly-and-efficiently-from-ongoing-experience"><a href="#By-constrast-biological-agents-exhibit-a-remark-able-ability-to-learn-quickly-and-efficiently-from-ongoing-experience" class="headerlink" title="By constrast, biological agents exhibit a remark-able ability to learn quickly and efficiently from ongoing experience :"></a>By constrast, biological agents exhibit a remark-able ability to learn quickly and efficiently from ongoing experience :</h3><ul><li>animals can learn to navigate and remember the location of food sources, discover and remember rewarding or aversive properties of novel objects and situations,etc.- often from a single exposere . </li></ul><h3 id="An-additional-beenefit-of-autonomous-learning-abilities"><a href="#An-additional-beenefit-of-autonomous-learning-abilities" class="headerlink" title="An additional beenefit of autonomous learning abilities"></a>An additional beenefit of autonomous learning abilities</h3><ul><li>In many tasks (e.g. object recognition , maze navigation ,etc. )</li><li><p>The bulk of fixed , unchanging structure in the task can be stored in the fixed knowledge of the agent,leaving only the changing , contingent parameters of the specific situation to be learned from experience. </p><ul><li>任务中大部分不改变的结构可以被存储智能体固定的知识中，只留下改变的，特殊场景的持续变化的参数，从经验中学习。</li></ul></li></ul><h3 id="As-a-result-learning-the-actual-specific-instance-of-the-task-at-hand-that-is-the-actual-latent-parameters-that-do-vary-across-multiple-instances-of-the-general-task-can-be-extremely-fast-requiring-few-or-even-a-single-experience-with-the-environment"><a href="#As-a-result-learning-the-actual-specific-instance-of-the-task-at-hand-that-is-the-actual-latent-parameters-that-do-vary-across-multiple-instances-of-the-general-task-can-be-extremely-fast-requiring-few-or-even-a-single-experience-with-the-environment" class="headerlink" title="As a result, learning the actual specific instance of the task at hand(that is , the actual latent parameters that do vary across multiple instances of the general task ) can be extremely fast requiring few or even a single experience with the environment."></a>As a result, learning the actual specific instance of the task at hand(that is , the actual latent parameters that do vary across multiple instances of the general task ) can be extremely fast requiring few or even a single experience with the environment.</h3><ul><li>因此， 学习当前任务的特定实例，可能非常快， 甚至只需要当前环境中单独的经验</li></ul><h3 id="Several-meta-learning-methods-have-been-proposed-to-train-agents-to-learn-autonomously"><a href="#Several-meta-learning-methods-have-been-proposed-to-train-agents-to-learn-autonomously" class="headerlink" title="Several meta-learning methods have been proposed to train agents to learn autonomously ."></a>Several meta-learning methods have been proposed to train agents to learn autonomously .</h3><ul><li>几种元学习方法以及被提出来训练智能体自动学习。</li></ul><h3 id="However-unlike-in-current-approaches-in-biological-brains-long-term-learning-is-thought-to-occur-primarily-through-synaptic-plasticity"><a href="#However-unlike-in-current-approaches-in-biological-brains-long-term-learning-is-thought-to-occur-primarily-through-synaptic-plasticity" class="headerlink" title="However , unlike in current approaches , in biological brains long-term learning is thought to occur primarily through synaptic plasticity ."></a>However , unlike in current approaches , in biological brains long-term learning is thought to occur primarily through synaptic plasticity .</h3><ul><li>The strengthening and weakening of connections between neurons as a result of neural activity . </li><li>as carefully tuned by evolution over millions of years to enable efficient learning during the lifetime of each individual. </li><li><p>While multiple forms of synaptic plasticity exist, many of them build upon the general principle known as Hebb’srule</p><ul><li>多数突触可塑性都是建立在Hebb规则上的</li></ul></li></ul><h3 id="Hebb’s-rule"><a href="#Hebb’s-rule" class="headerlink" title="Hebb’s rule"></a>Hebb’s rule</h3><ul><li><p>if a neuron repeatedly takes part in making another neuron fire, the connection between them is strengthened </p><ul><li>(often roughly summarized as “neurons that fire together , wire together”)</li></ul></li></ul><h3 id="Designing-neural-networks-with-plastic-connections-has-long-been-explored-with-evolutionary-algorithms-but-has-been-so-far-relatively-less-studied-in-deep-learning"><a href="#Designing-neural-networks-with-plastic-connections-has-long-been-explored-with-evolutionary-algorithms-but-has-been-so-far-relatively-less-studied-in-deep-learning" class="headerlink" title="Designing neural networks with plastic connections has long been explored with evolutionary algorithms , but has been so far relatively less studied in deep learning ."></a>Designing neural networks with plastic connections has long been explored with evolutionary algorithms , but has been so far relatively less studied in deep learning .</h3><h3 id="However-given-the-spectacular-results-of-gradient-descent-in-designing-traditional-plastic-neural-networks-for-complex-tasks-it-would-be-great-interest-to-expand-backpropagation-training-to-networks-with-plastic-connections-optimizing-through-gradient-descent-not-only-the-base-weights-but-also-the-amount-of-plasticity-in-each-connection"><a href="#However-given-the-spectacular-results-of-gradient-descent-in-designing-traditional-plastic-neural-networks-for-complex-tasks-it-would-be-great-interest-to-expand-backpropagation-training-to-networks-with-plastic-connections-optimizing-through-gradient-descent-not-only-the-base-weights-but-also-the-amount-of-plasticity-in-each-connection" class="headerlink" title="However, given the spectacular results of gradient descent in designing traditional -plastic neural networks for complex tasks, it would be great interest to expand backpropagation training to networks with plastic connections - optimizing through gradient descent not only the base weights , but also the amount of plasticity in each connection ."></a>However, given the spectacular results of gradient descent in designing traditional -plastic neural networks for complex tasks, it would be great interest to expand backpropagation training to networks with plastic connections - optimizing through gradient descent not only the base weights , but also the amount of plasticity in each connection .</h3><ul><li>然而，为一些复杂任务设计传统塑性神经网络时，</li></ul><h3 id="We-previously-demonstrate-the-theoretical-feasibility-and-analytical-derivability-of-this-approach"><a href="#We-previously-demonstrate-the-theoretical-feasibility-and-analytical-derivability-of-this-approach" class="headerlink" title="We previously demonstrate the theoretical feasibility and analytical derivability of this approach ."></a>We previously demonstrate the theoretical feasibility and analytical derivability of this approach .</h3><h3 id="Here-we-show-that-this-approach-can-train-large-networks-for-non-trivial-tasks"><a href="#Here-we-show-that-this-approach-can-train-large-networks-for-non-trivial-tasks" class="headerlink" title="Here we show that this approach can train large networks for non-trivial tasks ."></a>Here we show that this approach can train large networks for non-trivial tasks .</h3><h3 id="To-demonstrate-our-approach-we-apply-it-to-three-different-types-of-tasks-complex-pattern-memorization-including-natural-images"><a href="#To-demonstrate-our-approach-we-apply-it-to-three-different-types-of-tasks-complex-pattern-memorization-including-natural-images" class="headerlink" title="To demonstrate our approach , we apply it to three different types of tasks  : complex pattern memorization (including natural images )."></a>To demonstrate our approach , we apply it to three different types of tasks  : complex pattern memorization (including natural images ).</h3><h3 id="one-shot-classification-on-the-Omniglot-dataset-and-reinforcement-learning-in-a-maze-exploration-problem"><a href="#one-shot-classification-on-the-Omniglot-dataset-and-reinforcement-learning-in-a-maze-exploration-problem" class="headerlink" title="one-shot classification (on the Omniglot dataset), and reinforcement learning (in a maze exploration problem ) ."></a>one-shot classification (on the Omniglot dataset), and reinforcement learning (in a maze exploration problem ) .</h3><h3 id="We-show-that-plastic-networks-provide-competitive-results-on-Omniglot-improve-performance-in-maze-exploration-and-outperform-advanced-non-plastic-recurrent-networks-LSTMs-by-orders-of-magnitude-in-complex-pattern-memorization"><a href="#We-show-that-plastic-networks-provide-competitive-results-on-Omniglot-improve-performance-in-maze-exploration-and-outperform-advanced-non-plastic-recurrent-networks-LSTMs-by-orders-of-magnitude-in-complex-pattern-memorization" class="headerlink" title="We show that plastic networks provide competitive results on Omniglot, improve performance in maze exploration and outperform advanced non-plastic recurrent networks (LSTMs) by orders of magnitude in complex pattern memorization ."></a>We show that plastic networks provide competitive results on Omniglot, improve performance in maze exploration and outperform advanced non-plastic recurrent networks (LSTMs) by orders of magnitude in complex pattern memorization .</h3><h3 id="This-result-is-interesting-not-only-for-opening-up-a-new-avenue-of-investigation-in-gradient-based-neural-network-training-but-also-for-showing-that-meta-properties-of-neural-structures-normally-atrributed-to-evolution-or-a-priori-design-are-in-fact-amenable-to-gradient-descent-hinting-at-a-whole-class-of-heretofore-unimagine-meta-learning-algorithms"><a href="#This-result-is-interesting-not-only-for-opening-up-a-new-avenue-of-investigation-in-gradient-based-neural-network-training-but-also-for-showing-that-meta-properties-of-neural-structures-normally-atrributed-to-evolution-or-a-priori-design-are-in-fact-amenable-to-gradient-descent-hinting-at-a-whole-class-of-heretofore-unimagine-meta-learning-algorithms" class="headerlink" title="This result is interesting not only for opening up a new avenue of investigation in gradient-based neural network training , but also for showing that meta -properties of neural structures normally atrributed to evolution or a priori design are in fact amenable to gradient descent , hinting at a whole class of heretofore unimagine meta-learning algorithms"></a>This result is interesting not only for opening up a new avenue of investigation in gradient-based neural network training , but also for showing that meta -properties of neural structures normally atrributed to evolution or a priori design are in fact amenable to gradient descent , hinting at a whole class of heretofore unimagine meta-learning algorithms</h3><h2 id="Differentiable-plasticity"><a href="#Differentiable-plasticity" class="headerlink" title="Differentiable plasticity"></a>Differentiable plasticity</h2><h3 id="To-train-plastic-networks-with-backpropagation-a-plasticity-rule-must-be-specified"><a href="#To-train-plastic-networks-with-backpropagation-a-plasticity-rule-must-be-specified" class="headerlink" title="To train plastic networks with backpropagation , a plasticity rule must be specified."></a>To train plastic networks with backpropagation , a plasticity rule must be specified.</h3><ul><li><blockquote><p>为了使用反向传播训练塑性网络，必须指定一个塑性规则</p></blockquote></li></ul><h3 id="Here-we-choose-a-flexible-formulation-that-keeps-separate-plastic-and-non-plastic-components-for-each-connection"><a href="#Here-we-choose-a-flexible-formulation-that-keeps-separate-plastic-and-non-plastic-components-for-each-connection" class="headerlink" title="Here we choose a flexible formulation that keeps separate plastic and non-plastic components for each connection"></a>Here we choose a flexible formulation that keeps separate plastic and non-plastic components for each connection</h3><ul><li>在这里， 我们选择一个合适的公式，为每个连接保留塑性和非塑性组件。 </li></ul><h3 id="while-allowing-multiple-Hebbian-rules-to-be-easily-implemented-within-the-framework"><a href="#while-allowing-multiple-Hebbian-rules-to-be-easily-implemented-within-the-framework" class="headerlink" title="while allowing multiple Hebbian rules to be easily implemented within the framework."></a>while allowing multiple Hebbian rules to be easily implemented within the framework.</h3><ul><li>同时允许框架内能够轻松实现多个Hebbian rules </li></ul><h3 id="A-connection-between-any-two-neurons-i-and-j-has-both-a-fixed-component-and-plastic-component"><a href="#A-connection-between-any-two-neurons-i-and-j-has-both-a-fixed-component-and-plastic-component" class="headerlink" title="A connection between any two neurons $i$ and $j$ has both a fixed component and plastic component."></a>A connection between any two neurons $i$ and $j$ has both a fixed component and plastic component.</h3><ul><li>任何两个神经元i和j之间，都有一个固定不变的组件 和一个 塑性组件。 </li></ul><h3 id="The-fixed-part-is-just-a-traditional-connection-weight-w-i-j"><a href="#The-fixed-part-is-just-a-traditional-connection-weight-w-i-j" class="headerlink" title="The fixed part is just a traditional connection weight $w_{i,j}$,"></a>The fixed part is just a traditional connection weight $w_{i,j}$,</h3><ul><li>固定组件就是一个传统的连接权重 $w_{i,j}$, </li></ul><h3 id="The-plastic-part-is-stored-in-a-Hebbian-trace-Hebb-i-j-Which-varies-during-a-lifetime-according-to-ongoing-inputs-and-outputs-note-that-we-use-“lifetime”-and-“episode”-interchangeably"><a href="#The-plastic-part-is-stored-in-a-Hebbian-trace-Hebb-i-j-Which-varies-during-a-lifetime-according-to-ongoing-inputs-and-outputs-note-that-we-use-“lifetime”-and-“episode”-interchangeably" class="headerlink" title="The plastic part is stored in a Hebbian trace $Hebb_{i,j}$, Which varies during a lifetime according to ongoing inputs and outputs (note that we use “lifetime” and “episode” interchangeably.)"></a>The plastic part is stored in a <strong><em>Hebbian trace</em></strong> $Hebb_{i,j}$, Which varies during a lifetime according to ongoing inputs and outputs (note that we use “lifetime” and “episode” interchangeably.)</h3><ul><li>塑性组件被存储在一个 Hebbian  跟踪 $Hebb_{i,j}$里面， 在一个生命周期里面随着输入和输出变化。</li></ul><h3 id="In-the-simplest-case-studied-here-the-Hebbian-trace-is-simply-a-running-average-of-the-product-of-pre-and-post-synaptic-activity"><a href="#In-the-simplest-case-studied-here-the-Hebbian-trace-is-simply-a-running-average-of-the-product-of-pre-and-post-synaptic-activity" class="headerlink" title="In the simplest case studied here, the Hebbian trace is simply a running average of the product of pre- and post-synaptic activity ."></a>In the simplest case studied here, the Hebbian trace is simply a running average of the product of pre- and post-synaptic activity .</h3><h3 id="The-relative-importance-of-plastic-and-fixed-components-in-the-connection-is-structurally-determined-by-the-plasticity-coefficient-aerfa-i-j-Which-multiplies-the-Hebbian-trace-to-form-the-full-plastic-component-of-the-connection"><a href="#The-relative-importance-of-plastic-and-fixed-components-in-the-connection-is-structurally-determined-by-the-plasticity-coefficient-aerfa-i-j-Which-multiplies-the-Hebbian-trace-to-form-the-full-plastic-component-of-the-connection" class="headerlink" title="The relative importance of plastic and fixed components in the connection is structurally determined by the plasticity coefficient $\aerfa_{i,j}$, Which multiplies the Hebbian trace to form the full plastic component of the connection."></a>The relative importance of plastic and fixed components in the connection is structurally determined by the plasticity coefficient $\aerfa_{i,j}$, Which multiplies the Hebbian trace to form the full plastic component of the connection.</h3><ul><li>在连接中，塑性组件和固定组件的相对重要性是通过塑性系数结构化决定的， 塑性系数 是 Hebbian trace 相乘来形成连接的全局塑性组件。 </li></ul><h3 id="Thus-at-any-time-the-total-effective-weight-of-the-connection-between-neurons-i-and-j-is-the-sum-of-the-baseline-fixed-weight-w-i-j-，plus-the-hebbian-trace"><a href="#Thus-at-any-time-the-total-effective-weight-of-the-connection-between-neurons-i-and-j-is-the-sum-of-the-baseline-fixed-weight-w-i-j-，plus-the-hebbian-trace" class="headerlink" title="Thus, at any time , the total , effective weight of the connection between neurons i and j is the sum of the baseline(fixed) weight $w_{i,j}$，plus the hebbian trace"></a>Thus, at any time , the total , effective weight of the connection between neurons i and j is the sum of the baseline(fixed) weight $w_{i,j}$，plus the hebbian trace</h3><ul><li>因此， 在任何时候， 神经元i和j之间的所有有效权重， 是固定的权重$w_{i,j}$， 加上Hebbian trace$Hebb_{i,j}$ 乘以塑性系数。</li></ul><h3 id="The"><a href="#The" class="headerlink" title="The"></a>The</h3><ul><li>神经元j的输出 $x_j(t)$ 如下<br><img src="/2019/03/14/PLASTICITY-NEURAL-NETWORKS/1.png" alt=""></li><li>$\sigmma$ 是一个非线性函数 </li><li>input 代表所有给神经元j提供输入的神经元。 </li><li><p>通过这种方式， 取决于权值$w_{i,j}$ 和$\alpha_{i,j}$， </p><ul><li>如果$\alpha=0$ ,一个连接可以是完全fixed的</li><li>如果$w=0$ , 一个连接可以是没有fixed组件的完全plastic的。</li><li>或者一个连接有固定组件和塑性组件。 </li></ul></li></ul><h3 id="The-Hebbian-trace-Hebb-i-j"><a href="#The-Hebbian-trace-Hebb-i-j" class="headerlink" title="The Hebbian trace $Hebb_{i,j}$"></a>The Hebbian trace $Hebb_{i,j}$</h3><ul><li>在每个生命周期的开始初始化为0 </li><li>参数$w_{i,j}$和$\alpha_{i,j}$ ，在整个生命周期被保存，这个神经网络的结构化参数，通过梯度下降法优化， 在一个生命周期最大化期待的性能</li><li><p>$\elta$ ， 学习率， 也是神经网络的一个优化参数， 在本文中，所有的连接共享相同的$\elta$值。 它是整个网络的学习标量参数。</p><ul><li>elta,作为权重衰减项出现， 保证了Hebbian迹的失控正反馈。</li></ul></li><li><p>然而因为，这个权重衰减， hebb迹在输入缺少的情况下衰减到0</p></li><li>不幸的是， 其他更复杂的Hebb规则可以在刺激缺少的情况下，无限保持权重值的稳定性。</li><li>一个更有名的例子就是Oja’s 规则 ， 因此可以用Oja规则替换上面公式中的第二个公式<br><img src="/2019/03/14/PLASTICITY-NEURAL-NETWORKS/2.png" alt=""></li><li>这种方法可以训练神经网络形成持续任意时间的记忆。 </li><li>为了描述我们研究的灵活性， 我们证明了两个规则， 在下面报告的实验中</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><h3 id="这个实验被设计用来证明可微塑性实际上在一个元学习框架内起作用，-并且展示它提供了决定性的优势。"><a href="#这个实验被设计用来证明可微塑性实际上在一个元学习框架内起作用，-并且展示它提供了决定性的优势。" class="headerlink" title="这个实验被设计用来证明可微塑性实际上在一个元学习框架内起作用， 并且展示它提供了决定性的优势。"></a>这个实验被设计用来证明可微塑性实际上在一个元学习框架内起作用， 并且展示它提供了决定性的优势。</h3><h3 id="模式记忆-：-二元模式"><a href="#模式记忆-：-二元模式" class="headerlink" title="模式记忆 ： 二元模式"></a>模式记忆 ： 二元模式</h3><ul><li>为了描述这些不同的塑性方法，我们首先把它应用到任意高维模式的快速记忆集合中去， 并且当这些集合暴露出了局部和降维的版本，就重建这些模式。</li><li>可以执行这个任务的神经网络被称为 内容可寻址记忆， 或者 自动联想神经网络</li><li>这个任务是一个有用的测试， 因为使用Hebbian plastic 连接手动设计的循环神经网络可以成功解决这个二元模式</li><li>因此，如果可微的塑性网络有任何帮助， 它也应该嫩自动解决这个问题，这个问题就是， 自动设计可以执行已经存在的手动设计神经网络可以完成的任务 的神经网络</li><li>图1描绘了这个任务的一个生命周期， 该网络连续显示一组5个二维模式。<br><img src="/2019/03/14/PLASTICITY-NEURAL-NETWORKS/3.png" alt=""></li><li>每个二元模式由1000个元素组成， 每一个不是1就是-1， 分别用红色和蓝色表示。 </li><li><p>每个模式展示了10个周期步骤</p><ul><li>在演示文稿之间有输入为0的三个周期步骤</li><li>所有的模式序列通过3次随机顺序展示。</li><li>然后，通过将所述模式的一半比特设置为零，随机选择其中的一个模式并对其进行降级。</li><li>然后将这种退化的模式作为网络的输入</li></ul></li><li><p>网络的任务是输出重建的正确完整模式，在它的记忆中绘制完成损失的降维模式（底部的浅蓝色和红色）</p></li><li>图1下面的架构是完全循环身价还哦，每一个模式元素都有一个神经元， 加上一个固定的输出神经元（偏差），对于所有的1001个神经元， 输入模式是通过固定每个神经元的只，到模式中对应的元素的值来提供的， 如果这个值不是0 的话； </li><li>对于降维模式中的0值输入，对应的神经元没收到模式输入， 并且从侧面连接中唯一地得到他们的输入。 他们必须重建正确的期待的输出结果。 </li><li>输出直接从活跃神经元中读取。 </li><li>神经网络额性能仅仅在最后一步中被评价</li><li>通过计算累计squared误差， 最终的神经网络输出和正确的模式 </li><li>然后根据反向传播计算了 $w$和$\alpha$ 的梯度误差</li><li>然后这些系数通过一个Adam solver 使用学习率 0.001解决了。 </li><li>在这个实验中， 我们使用了简单衰减的Hebbian 公式来更新hebbian迹</li><li>注意，这个神经网络有两个训练参数 （$w$、$\alpha$），加起来是 1001<em>1001</em>2个训练参数。 </li><li>在大概200个周期的时候，误差率缩减到不再变化</li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Plastic-NN&quot;&gt;&lt;a href=&quot;#Plastic-NN&quot; class=&quot;headerlink&quot; title=&quot;Plastic NN&quot;&gt;&lt;/a&gt;Plastic NN&lt;/h1&gt;&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Thinking Note during Reading SSL</title>
    <link href="http://yoursite.com/2019/03/14/THINKING-NOTE-DURING-READING-SSL/"/>
    <id>http://yoursite.com/2019/03/14/THINKING-NOTE-DURING-READING-SSL/</id>
    <published>2019-03-14T03:04:06.000Z</published>
    <updated>2019-03-14T12:33:05.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SSL-Survey"><a href="#SSL-Survey" class="headerlink" title="SSL Survey"></a>SSL Survey</h1><h2 id="监督学习分类中的限制"><a href="#监督学习分类中的限制" class="headerlink" title="监督学习分类中的限制"></a>监督学习分类中的限制</h2><ul><li>需要足够的标签</li><li>学习到的分类器只能区分， 被训练数据覆盖了的实例</li></ul><h2 id="为了解决上面的问题已经有了的一些方案"><a href="#为了解决上面的问题已经有了的一些方案" class="headerlink" title="为了解决上面的问题已经有了的一些方案"></a>为了解决上面的问题已经有了的一些方案</h2><ul><li><p>open set recognition methods</p><blockquote><p>这种方法的分类器不能决定出，没有见过的实例属于什么类别。 </p></blockquote></li><li><p>这些方法的缺陷</p><blockquote><p>如果测试实例， 是没有标签的，在模型学习中没有见过的类别， 学习到的分类器不能决定这些测试实例的类别标签。 </p></blockquote></li></ul><h2 id="一些流行的应用场景"><a href="#一些流行的应用场景" class="headerlink" title="一些流行的应用场景"></a>一些流行的应用场景</h2><p>（那些需要分类器拥有决定测试实例的类别标签的能力的情况。 ）</p><ul><li>目标类别庞大的情景 </li><li>目标类别稀疏的情景</li><li>目标类别随着时间变化</li><li>在一些特殊性的任务中， 需要花费昂贵代价得到带标签的实例 </li></ul><p>为了解决这个问题， ZSL 被提出了</p><h1 id="ZSL-（zero-shot-learning）"><a href="#ZSL-（zero-shot-learning）" class="headerlink" title="ZSL （zero-shot learning）"></a>ZSL （zero-shot learning）</h1><ul><li>目标： 区分出没有标签的实例。</li><li>应用范围： </li><li>计算机视觉</li><li>自然语言处理</li><li>普适计算</li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/1.png" alt="图1"></p><ul><li>它的特征空间由两部分组成： 带标签的训练实例 +  不带标签的测试实例<ul><li>每一个实例通常由一个向量vector代表它。 </li><li>每一个实例假设只属于一个类别。 </li></ul></li></ul><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>$S$  是Seen Classes 可见类别的数据集合。<br>$U$  是Unseen Classes 不可见类别的数据集合。</p><p>上面两个集合不相交。 </p><p>$X$  是D维特征空间。<br>$D^{tr}$  是带标签的可见类别训练集合，它等于特征空间X×可见类别数据集S 的乘积集合。<br>$X^{te}$  是测试集合， 它是特征空间中的元素。 </p><h2 id="特征空间"><a href="#特征空间" class="headerlink" title="特征空间"></a>特征空间</h2><p><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/2.png" alt="图2"></p><ul><li>特征空间分类两部分： 带标签的训练实例  +  不带标签的测试实例<ul><li>带标签的训练实例集，由 $D^{tr}$ 表示， 元素代表已知类</li><li>不带标签的测试实例集，由$X^{te}$ 表示， 元素代表了未知类</li></ul></li></ul><h2 id="ZSL的定义"><a href="#ZSL的定义" class="headerlink" title="ZSL的定义"></a>ZSL的定义</h2><ul><li>Zero-Shot Learning<ul><li>给出已知类$S$ 的带标签训练实例集$D^{tr}$ ,ZSL的目标是学习出一个分类器$f$ D维特征空间到未知类U 的映射，这样就可以用来分类未知分类集合$U$中的测试实例$X^{te}$（可以预测$Y^{te}$）</li><li>ZSL是一种迁移学习</li><li>迁移学习可以分为：同类迁移学习(homogenneous)，和异类迁移学习（heterogeneous）<br><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/3.png" alt="图3"><ul><li>同类迁移学习： 源和目标的特征空间及标签空间是相同的</li><li>异类迁移学习： 源和目标的特征空间和标签空间是不同的</li></ul></li><li>ZSL中，源和目标的特征空间相同，但是标签空间是不相同的，分为S和U，已知类和未知类。<br><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/4.png" alt="图4"></li><li>所以ZSL属于迁移学习中的异类迁移学习，</li><li>ZSL和不同标签空间的异类迁移学习（HTL-DLS）很类似，但是区别在于</li><li>区别在于 ， HTL-DLS 的target label space 有带标签的实例，而ZSL没有。 </li></ul></li></ul><h1 id="辅助信息-Auxiliary-information"><a href="#辅助信息-Auxiliary-information" class="headerlink" title="辅助信息 Auxiliary information"></a>辅助信息 Auxiliary information</h1><p>这些辅助信息应该包含所有的 未知类（Unseen Classes），同时， 辅助信息应该对应特征空间的实例。<br>现有的ZSL方法中包含的辅助信息同上是一些语义信息（Semantic Information）<br>辅助信息构造了一个包含已知类(Seen)和未知类（Unseen）的空间</p><ul><li><p>我们假设$\Gamma$ 是这个语义空间，是M维的。 </p><ul><li>$t_i^s\in \Gamma$ 是已知类$c_i^s$的原型</li><li><p>$t_i^u\in \Gamma$ 是未知类$c_i^u$的原型</p></li><li><p>$T^s$</p></li><li><p>$T^u$</p></li><li><p>$\pi$ 是已知类S和未知类U的并集到语义空间T的映射， 这个函数的输入是一种类别标签， 输出对应的类别原型<br><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/5.png" alt="图5"></p><ul><li>在特征空间之外，构造了一个语义空间，</li><li>语义空间中，包含了 已知类的原型（seen class prototype）和未知类的原型。 </li><li>语义空间中的每种类别有对应的向量表示（vector representation）</li><li>特征空间中的每一个实例由对应的向量表示。</li></ul></li></ul></li></ul><p><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/t1.png" alt="表1">本文使用的数学符号。 </p><h1 id="Learning-Settings"><a href="#Learning-Settings" class="headerlink" title="Learning Settings"></a>Learning Settings</h1><ul><li>三种学习设置：<ul><li>CIII 类别归纳实例归纳设置 （Class-Inductive Instance-Inductive Setting）：<ul><li>只有带标签的实例$D^{tr}$ 和已知类原型$T^s$ 用在模型学习中。 </li></ul></li><li>CTII 类别转换实例归纳设置 （Class-Transductive Instance-Inductive (CTII) Setting）：<ul><li>带标签的训练实例$D^tr$ 、已知类原型$T^s$ 、未知类原型$T^u$ 被用在模型训练中。 </li></ul></li><li>CTIT 类别转换实例转换设置 （Class-Transductive Instance-Transductive (CTIT) Setting）：        <ul><li>带标签的训练实例$D^tr$、已知类原型$T^s$ 、未知类原型$T^u$、不带标签的测试实例$X^{te}$ </li></ul></li></ul></li></ul><h1 id="Domain-shift-域转换现象"><a href="#Domain-shift-域转换现象" class="headerlink" title="Domain shift 域转换现象"></a>Domain shift 域转换现象</h1><pre><code>- 在ZSL学习中，在接受了测试实例之后， 使用训练实例的模型的性能将会下降的现象。 </code></pre><h1 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h1><ul><li>将ZSL方法分为三类：<ul><li>One-Order transformation</li><li>Two-Order transformation</li><li>high-Order transformation</li></ul></li><li>本文的重点是对现有的ZSL方法给出了评估。 <ul><li>我们整理了ZSL中不同的语义空间和方法<br><img src="/2019/03/14/THINKING-NOTE-DURING-READING-SSL/6.png" alt="图6"><ul><li>语义空间分为， 设计语义空间，和习得语义空间<ul><li>设计语义空间分为， 属性空间、字典空间、关键词空间。 </li><li>习得语义空间分为， 标签嵌入空间、文本嵌入空间、图像代表空间。</li></ul></li><li>零向量学习方法分为： 基于分类器的方法、基于实例的方法。 <ul><li>基于分类器的方法分为： 通信方法、关系方法、组合方法</li><li>基于实例的方法分为： 规划方法、 实例借入方法、重组方法</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SSL-Survey&quot;&gt;&lt;a href=&quot;#SSL-Survey&quot; class=&quot;headerlink&quot; title=&quot;SSL Survey&quot;&gt;&lt;/a&gt;SSL Survey&lt;/h1&gt;&lt;h2 id=&quot;监督学习分类中的限制&quot;&gt;&lt;a href=&quot;#监督学习分类中的限制&quot;
      
    
    </summary>
    
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>SSL Review -2</title>
    <link href="http://yoursite.com/2019/03/10/SSL-REVIEW-2-%20Overview%20of%20Zero-Shot%20Learning/"/>
    <id>http://yoursite.com/2019/03/10/SSL-REVIEW-2- Overview of Zero-Shot Learning/</id>
    <published>2019-03-10T02:10:25.000Z</published>
    <updated>2019-03-11T02:01:53.712Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview-of-Zero-Shot-learning"><a href="#Overview-of-Zero-Shot-learning" class="headerlink" title="Overview of Zero-Shot learning"></a>Overview of Zero-Shot learning</h1><ul><li><p>the seen classes</p><ul><li>the classes covered by labeled training instances in the feauture space</li></ul></li><li><p>the unseen classes</p><ul><li>unlabeled testing instances in the feature space which belong to another set of classes</li></ul></li><li><p>the feature space</p><ul><li>a real number space</li><li>each instance is represented as a vector within it</li><li>each instance is usually assumed to belong to one class</li></ul></li><li><p><strong>definition</strong></p><ul><li>the Set of Seen Classes:<pre><code>$$S=\\{c_{i}^{s}\|i=1,……,N_{s}\\}$$</code></pre><ul><li>a seen classes:$c_{i}^{j}$</li></ul></li><li>the Set of Unseen Classes:<pre><code>$$U=\\{c_{i}^{u}\\|i=1,N_{u}\\}$$</code></pre></li><li>an unseen class:$C_{i}^{u}$</li><li>Denote that ：${S}\bigcap{U}=\varnothing$</li><li>The Feature Space:$X$, which is $D-dimensional$: $R^{D}$</li><li><p>The set of labeled training instances belonging to seen classes:</p><p>$$D^{tr}=\{(x_{i}^{tr},y_{i}^{tr})\in X\times S\}$$</p></li><li><p>for each labeled instance $(x_i^{tr},y_i^{tr})$,</p><ul><li>$x_i^{tr}$ ：the instance in the feature space ，</li><li>$y_i^{tr}$： the corresponding class label .</li></ul></li><li>The set of testing instances:<br>  $$X^{te}=\{x_i^{te}\in X\}<em>{i=1}^{N</em>{te}}$$<br>  where each $x_i^{te}$ is a testing instance in the feature space.</li><li>The corresponding class labels for $X^{te}$:<br>  $$Y^{te}=\{y_i^{te}\in U\}<em>{i=1}^{N</em>{te}}$$<br>  which are able to be predicted.<h2 id="Definition-1-1-Zero-Shot-Learning"><a href="#Definition-1-1-Zero-Shot-Learning" class="headerlink" title="Definition 1.1 (Zero-Shot Learning)"></a>Definition 1.1 (Zero-Shot Learning)</h2></li></ul></li><li>Given labeled training instances $D^{tr}$ belonging to the seen classes $S$,zero-shot learning aims to learn a classifier $f^u(\cdot):X\rightarrow U$  that can classify testing instances $X^{te}$(i.e. to predict $Y^{te}$) belonging to the unseen classes $U$.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Overview-of-Zero-Shot-learning&quot;&gt;&lt;a href=&quot;#Overview-of-Zero-Shot-learning&quot; class=&quot;headerlink&quot; title=&quot;Overview of Zero-Shot learning&quot;&gt;
      
    
    </summary>
    
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>SSL Review -1</title>
    <link href="http://yoursite.com/2019/03/09/SSL-REVIEW-1/"/>
    <id>http://yoursite.com/2019/03/09/SSL-REVIEW-1/</id>
    <published>2019-03-09T14:14:25.000Z</published>
    <updated>2019-03-09T14:18:03.209Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小样本综述：A-Survey-of-Zero-Shot-Learning-Settings-Methods-and-Applications"><a href="#小样本综述：A-Survey-of-Zero-Shot-Learning-Settings-Methods-and-Applications" class="headerlink" title="小样本综述：A Survey of Zero-Shot Learning: Settings, Methods,and Applications"></a>小样本综述：A Survey of Zero-Shot Learning: Settings, Methods,and Applications</h1><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><h3 id="Classify-zero-short-learning-according-to-the-data-utilized-in-model-optimization"><a href="#Classify-zero-short-learning-according-to-the-data-utilized-in-model-optimization" class="headerlink" title="Classify zero-short learning according to the data utilized in model optimization"></a>Classify zero-short learning according to the data utilized in model optimization</h3><h3 id="different-semantic-spaces-adopted-in-existing-zero-shot-learning-works"><a href="#different-semantic-spaces-adopted-in-existing-zero-shot-learning-works" class="headerlink" title="different semantic spaces adopted in existing zero-shot learning works"></a>different semantic spaces adopted in existing zero-shot learning works</h3><h3 id="categorize-existing-zero-shot-learning-methods"><a href="#categorize-existing-zero-shot-learning-methods" class="headerlink" title="categorize existing zero-shot learning methods"></a>categorize existing zero-shot learning methods</h3><h3 id="different-applications-of-zero-shot-learning"><a href="#different-applications-of-zero-shot-learning" class="headerlink" title="different applications of zero-shot learning"></a>different applications of zero-shot learning</h3><h3 id="future-research-directions-of-zero-shot-learning"><a href="#future-research-directions-of-zero-shot-learning" class="headerlink" title="future research directions of zero-shot learning"></a>future research directions of zero-shot learning</h3><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Resttrictions-for-Supervised-classification"><a href="#Resttrictions-for-Supervised-classification" class="headerlink" title="Resttrictions for Supervised classification"></a>Resttrictions for Supervised classification</h3><ul><li>1、sufficient labeled training instances are needed for each class </li><li>2、the learned classifier can only classify the instances belonging to classes covered by the training data </li><li>3、 it lacks the ability to deal with previously unseen classes </li><li>in practical applications , there may not be sufficient training instances for each class.<br>there could  also be situations in which the classes not covered by the training instances appear in the testing instances. </li></ul><h3 id="Method-to-deal-with-the-problems"><a href="#Method-to-deal-with-the-problems" class="headerlink" title="Method to deal with the problems"></a>Method to deal with the problems</h3><ul><li><p>few-shot learning methods<br>one-shot learing methods</p><ul><li>in these methods , while learning classifiers for the classes with few instances , knowledge contained in instances of other classes is utilized . </li></ul></li><li><p>open set recognition methods ,</p><ul><li>when learning classifier with the training data , the fact of unseen classes existing is taken in consideration </li><li>the learned classifier can  determine whether a testing instance belong to the unseen classes , but it cannot determine which specific unseen class the instance belongs to.</li></ul></li><li><p>the cumulative learning methods<br>the class-incremental learning methods </p><ul><li>proposed for problems in which labeled instances belonging to some previously unseen classes progressively appear after model learning .</li><li>the learned classifier can be adapted with these newly available labeled instances to be able to classify classes covered by them</li></ul></li><li><p>the open world recognition methods </p><ul><li>follow the process of “unseen classes detection , labeled instances of unseen classes acquisition, and model adaption “</li><li>adapt the classifier to be able to classify previously unseen classes with the acquired labeled instances belonging to them</li></ul></li><li><p>disadvantage of the methods under the above learning paradigms</p><ul><li>if the testing instances belong to unseen classes that have no available labeled instances during the model learning . </li><li>the learned classifier cannot determine the class labels of them. </li><li>However , in many practical applications , we need the classifier to have the ability to determine the class labels for the instances belonging to these classes .</li></ul></li><li><p>Some popular application scenarios </p><ul><li><p>The number of target classes is large.</p><ul><li>example: activity recognition</li></ul></li><li><p>Target classes are rare.</p><ul><li>example: fine-grained object classification , recognize flowers of different breeds. </li></ul></li><li><p>Target classes change over time.</p><ul><li>example: recognizing images of products belonging to a certain style and brand. </li></ul></li><li><p>In some particular tasks which is expensive to obtain labeled instances. </p></li></ul></li><li><p>For a  classifier </p><ul><li>it is important for it to have the ability to determine the class label of instances belonging to these classes . </li><li>to solve this problem ,zero-shot learning is proposed .</li></ul></li><li><p>Zero-shot learning</p><ul><li>aim to classify instances belong to the classes that have no labeled instances. </li></ul></li></ul><h3 id="Overview-of-Zero-Shot-learning"><a href="#Overview-of-Zero-Shot-learning" class="headerlink" title="Overview of Zero-Shot learning"></a>Overview of Zero-Shot learning</h3><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;小样本综述：A-Survey-of-Zero-Shot-Learning-Settings-Methods-and-Applications&quot;&gt;&lt;a href=&quot;#小样本综述：A-Survey-of-Zero-Shot-Learning-Settings-Meth
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SLAM - PAPER - 1</title>
    <link href="http://yoursite.com/2019/03/04/SLAM-PAPER/"/>
    <id>http://yoursite.com/2019/03/04/SLAM-PAPER/</id>
    <published>2019-03-04T07:01:14.000Z</published>
    <updated>2019-03-04T07:45:00.863Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features"><a href="#Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features" class="headerlink" title="Matching-range-constrained real-time loop closure detection with CNNs features"></a>Matching-range-constrained real-time loop closure detection with CNNs features</h2><p>具有匹配范围约束的带重复特征的实时环路闭包检测</p><ul><li><p>abstract</p><ul><li>loop closure detection 闭环检测LCD</li><li>DCNNs</li><li><p>Some researchers</p><ul><li>pre-trained CNNs model</li><li>generating an image representation</li><li>appropriate for visual LCD in SLAM</li></ul></li><li><p>Differences and Challenges between Simple Computer Vision  &amp; Robotic Application</p><ul><li>adjacent images more resemblance</li><li>real-time performance</li></ul></li><li><p>in this paper</p><ul><li><p>making use of the feature generated by CNNs layers</p><ul><li>to implement LCD in real environment</li></ul></li><li><p>the first problem</p><ul><li>provide a value to limit the matching range of images</li></ul></li><li><p>better results</p></li><li><p>improve the real-time performance</p><ul><li>using an efficient feature compression method</li></ul></li></ul></li></ul></li><li><p>background</p><ul><li><p>SLAM algorithm aims</p><ul><li>map an unknown environment</li><li>while simultaneously localizing the robot</li></ul></li><li><p>LCD</p><ul><li>determine whether a robot is back to previously visited location</li><li>correcting the accumulate error is critical for building a consistent map</li><li>One of the most essential techniques in SLAM</li></ul></li><li><p>develop a LCD algorithm</p><ul><li><p>one class of popular and successful techniques</p><ul><li>based on  Matching the current view of the robot with</li><li>Those Corresponding to previously visited locations in the robot map</li><li><p>OTHER</p><ul><li>image matching problem</li></ul></li><li><p>STEPS</p><ul><li>image description</li><li>similarity measurement</li></ul></li><li><p>The state-of-the-art Algorithms</p><ul><li><p>image description</p><ul><li><p>BoW ： bag-of-words model</p><ul><li><p>clusters the visual feature descriptors in images</p><ul><li><p>visual features (Success)</p><ul><li>SIFT</li><li>Surf</li></ul></li></ul></li><li><p>builds the dictionary</p></li><li>find the corresponding words of image</li></ul></li></ul></li><li><p>similarity measurement</p></li></ul></li></ul></li><li><p>Challenges still remain</p><ul><li>in dynamical and large-scale environment</li><li><p>long period of time</p><ul><li>day</li><li>week</li><li>months</li></ul></li><li><p>dramatic condition change</p></li><li>viewpoint change over time</li><li><p>bad news</p><ul><li>the hand-craft methods can not deal with these situations very well</li></ul></li><li><p>good news</p><ul><li>in recent ML and CV conference</li><li>the features generated by convolutional neural networks outperform well in visual recognition classification and detection applications</li></ul></li></ul></li><li><p>advantage of CNNs</p><ul><li>it has been demonstrated to be versatile and transferable</li><li>even though they were trained on a very specific target task</li><li>they can be successfully used to solving different problems</li><li>and may outperform traditional hand-craft features</li></ul></li></ul></li><li><p>Two Challenges appear while we use these features generated by CNNs in pratical environment</p><ul><li><p>Firstly , the adjacent images in the data-set of LCD might have more resemblance  than the images that really form the loop closure</p><ul><li>#?为什么相邻图像比真实构成闭环检测的图像更相似？</li><li>the algorithm tends to identify the adjacent images  as loop closure</li></ul></li><li><p>Secondly , the feature matching is computationally intensive , because the dimension of features generated by CNNs may be very large .</p></li></ul></li></ul></li></ul><p>LCD have to compare the current image to large amount of pre-captured images</p><pre><code>        - 太大的计算量不利于实时性- in this paper    - two solution        - firstly            - provide matching range of candidate images                - #将匹配到一张图片，变成匹配到相似的图片范围里去        - secondly            - a efficient feature compression method                - #有效在于，通过压缩了CNN层得到的图像特征，这样处理的图像变小，处理速度快一点就能提高实时性， （临界性能减小）            - to reduce dimension of feature generated by CNNs</code></pre><h2 id="ViNS-Mono"><a href="#ViNS-Mono" class="headerlink" title="ViNS-Mono"></a>ViNS-Mono</h2><h2 id="遥感影像道路提取研究"><a href="#遥感影像道路提取研究" class="headerlink" title="遥感影像道路提取研究"></a>遥感影像道路提取研究</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>首先利用各种特征提取方法提取有用特征</li><li>应用各种方法找出满足道路特征的道路</li><li>最后对道路提取结果进行后期优化处理得到最终道路提取结果</li></ul><h3 id="目前的方法"><a href="#目前的方法" class="headerlink" title="目前的方法"></a>目前的方法</h3><ul><li>特征提取</li><li><p>道路提取</p><ul><li>1、同时包含道路拓扑结构信息和宽度信息的提取</li><li>2、只提取出道路中心线的拓扑结构信息</li></ul></li></ul><h2 id="Long-Range-Traversable-Region-Detection-Based-on-Superpixels"><a href="#Long-Range-Traversable-Region-Detection-Based-on-Superpixels" class="headerlink" title="Long Range Traversable Region Detection Based on Superpixels"></a>Long Range Traversable Region Detection Based on Superpixels</h2><p>Clustering for Mobile Robots<br>可达性</p><h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><ul><li>— Traversable region detection</li><li><p>i传统方法缺点</p><ul><li>only  short range traversable regions can be detected</li><li><p>原因</p><ul><li>the limited image resolution and baseline of stereo vision</li></ul></li></ul></li><li><p>本文方法</p><ul><li>detect long range traversable regions without using any supervised or self-supervised learning process</li><li>Superpixels clustering algorithm</li><li>superpixels are  clustered using an improved spectral clustering algorithm to segment the image</li><li><p>integrating short range traversable region detection : u-v-disparity</p><ul><li>then the traversable region can be extended to long range naturally</li></ul></li></ul></li><li><p>result</p><ul><li>works well in different outdoor environments</li><li>detecting range can be improved greatly</li></ul></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>可达性</p><ul><li>regions that do not contain geometric obstacles</li></ul></li><li><p>采集信息</p><ul><li>ultrasonicsensor</li><li><p>stereo vision</p><ul><li>measure the ranges to objects</li><li>by calculating disparities between stereo images</li></ul></li><li><p>laser scanners</p></li></ul></li><li><p>key</p><ul><li>After acquiring the disparities ,traversable regions or obstacles can be detected robustly and efficiently<br>(using a series of approaches based on u-v-disparties )</li></ul></li><li><p>V-disparity</p><ul><li><p>Aim</p><ul><li>detect Obstacles</li></ul></li><li><p>(u,v)</p><ul><li>坐标</li></ul></li><li><p>ways</p><ul><li>by accumulating pixels with the same disparity value d in each row , a v-disparity image (d,v) was build</li><li><p>perpendicular obstacles can be mapped to vertical lines</p><ul><li>pixel intensity  represents  the width of obstacles</li></ul></li><li><p>the traversable region modelled as a succession of planes can be projected as slanted line segment</p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features&quot;&gt;&lt;a href=&quot;#Matching-range-constrained-real-time-loop-
      
    
    </summary>
    
      <category term="SLAM" scheme="http://yoursite.com/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>SLAM - PAPER - 1</title>
    <link href="http://yoursite.com/2019/03/04/SLAM-PAPER-1/"/>
    <id>http://yoursite.com/2019/03/04/SLAM-PAPER-1/</id>
    <published>2019-03-04T07:01:14.000Z</published>
    <updated>2019-03-10T06:47:03.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features"><a href="#Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features" class="headerlink" title="Matching-range-constrained real-time loop closure detection with CNNs features"></a>Matching-range-constrained real-time loop closure detection with CNNs features</h2><p>具有匹配范围约束的带重复特征的实时环路闭包检测</p><ul><li><p>abstract</p><ul><li>loop closure detection 闭环检测LCD</li><li>DCNNs</li><li><p>Some researchers</p><ul><li>pre-trained CNNs model</li><li>generating an image representation</li><li>appropriate for visual LCD in SLAM</li></ul></li><li><p>Differences and Challenges between Simple Computer Vision  &amp; Robotic Application</p><ul><li>adjacent images more resemblance</li><li>real-time performance</li></ul></li><li><p>in this paper</p><ul><li><p>making use of the feature generated by CNNs layers</p><ul><li>to implement LCD in real environment</li></ul></li><li><p>the first problem</p><ul><li>provide a value to limit the matching range of images</li></ul></li><li><p>better results</p></li><li><p>improve the real-time performance</p><ul><li>using an efficient feature compression method</li></ul></li></ul></li></ul></li><li><p>background</p><ul><li><p>SLAM algorithm aims</p><ul><li>map an unknown environment</li><li>while simultaneously localizing the robot</li></ul></li><li><p>LCD</p><ul><li>determine whether a robot is back to previously visited location</li><li>correcting the accumulate error is critical for building a consistent map</li><li>One of the most essential techniques in SLAM</li></ul></li><li><p>develop a LCD algorithm</p><ul><li><p>one class of popular and successful techniques</p><ul><li>based on  Matching the current view of the robot with</li><li>Those Corresponding to previously visited locations in the robot map</li><li><p>OTHER</p><ul><li>image matching problem</li></ul></li><li><p>STEPS</p><ul><li>image description</li><li>similarity measurement</li></ul></li><li><p>The state-of-the-art Algorithms</p><ul><li><p>image description</p><ul><li><p>BoW ： bag-of-words model</p><ul><li><p>clusters the visual feature descriptors in images</p><ul><li><p>visual features (Success)</p><ul><li>SIFT</li><li>Surf</li></ul></li></ul></li><li><p>builds the dictionary</p></li><li>find the corresponding words of image</li></ul></li></ul></li><li><p>similarity measurement</p></li></ul></li></ul></li><li><p>Challenges still remain</p><ul><li>in dynamical and large-scale environment</li><li><p>long period of time</p><ul><li>day</li><li>week</li><li>months</li></ul></li><li><p>dramatic condition change</p></li><li>viewpoint change over time</li><li><p>bad news</p><ul><li>the hand-craft methods can not deal with these situations very well</li></ul></li><li><p>good news</p><ul><li>in recent ML and CV conference</li><li>the features generated by convolutional neural networks outperform well in visual recognition classification and detection applications</li></ul></li></ul></li><li><p>advantage of CNNs</p><ul><li>it has been demonstrated to be versatile and transferable</li><li>even though they were trained on a very specific target task</li><li>they can be successfully used to solving different problems</li><li>and may outperform traditional hand-craft features</li></ul></li></ul></li><li><p>Two Challenges appear while we use these features generated by CNNs in pratical environment</p><ul><li><p>Firstly , the adjacent images in the data-set of LCD might have more resemblance  than the images that really form the loop closure</p><ul><li>#?为什么相邻图像比真实构成闭环检测的图像更相似？</li><li>the algorithm tends to identify the adjacent images  as loop closure</li></ul></li><li><p>Secondly , the feature matching is computationally intensive , because the dimension of features generated by CNNs may be very large .</p></li></ul></li></ul></li></ul><p>LCD have to compare the current image to large amount of pre-captured images</p><pre><code>        - 太大的计算量不利于实时性- in this paper    - two solution        - firstly            - provide matching range of candidate images                - #将匹配到一张图片，变成匹配到相似的图片范围里去        - secondly            - a efficient feature compression method                - #有效在于，通过压缩了CNN层得到的图像特征，这样处理的图像变小，处理速度快一点就能提高实时性， （临界性能减小）            - to reduce dimension of feature generated by CNNs</code></pre><h2 id="ViNS-Mono"><a href="#ViNS-Mono" class="headerlink" title="ViNS-Mono"></a>ViNS-Mono</h2><h2 id="遥感影像道路提取研究"><a href="#遥感影像道路提取研究" class="headerlink" title="遥感影像道路提取研究"></a>遥感影像道路提取研究</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>首先利用各种特征提取方法提取有用特征</li><li>应用各种方法找出满足道路特征的道路</li><li>最后对道路提取结果进行后期优化处理得到最终道路提取结果</li></ul><h3 id="目前的方法"><a href="#目前的方法" class="headerlink" title="目前的方法"></a>目前的方法</h3><ul><li>特征提取</li><li><p>道路提取</p><ul><li>1、同时包含道路拓扑结构信息和宽度信息的提取</li><li>2、只提取出道路中心线的拓扑结构信息</li></ul></li></ul><h2 id="Long-Range-Traversable-Region-Detection-Based-on-Superpixels"><a href="#Long-Range-Traversable-Region-Detection-Based-on-Superpixels" class="headerlink" title="Long Range Traversable Region Detection Based on Superpixels"></a>Long Range Traversable Region Detection Based on Superpixels</h2><p>Clustering for Mobile Robots<br>可达性</p><h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><ul><li>— Traversable region detection</li><li><p>i传统方法缺点</p><ul><li>only  short range traversable regions can be detected</li><li><p>原因</p><ul><li>the limited image resolution and baseline of stereo vision</li></ul></li></ul></li><li><p>本文方法</p><ul><li>detect long range traversable regions without using any supervised or self-supervised learning process</li><li>Superpixels clustering algorithm</li><li>superpixels are  clustered using an improved spectral clustering algorithm to segment the image</li><li><p>integrating short range traversable region detection : u-v-disparity</p><ul><li>then the traversable region can be extended to long range naturally</li></ul></li></ul></li><li><p>result</p><ul><li>works well in different outdoor environments</li><li>detecting range can be improved greatly</li></ul></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>可达性</p><ul><li>regions that do not contain geometric obstacles</li></ul></li><li><p>采集信息</p><ul><li>ultrasonicsensor</li><li><p>stereo vision</p><ul><li>measure the ranges to objects</li><li>by calculating disparities between stereo images</li></ul></li><li><p>laser scanners</p></li></ul></li><li><p>key</p><ul><li>After acquiring the disparities ,traversable regions or obstacles can be detected robustly and efficiently<br>(using a series of approaches based on u-v-disparties )</li></ul></li><li><p>V-disparity</p><ul><li><p>Aim</p><ul><li>detect Obstacles</li></ul></li><li><p>(u,v)</p><ul><li>坐标</li></ul></li><li><p>ways</p><ul><li>by accumulating pixels with the same disparity value d in each row , a v-disparity image (d,v) was build</li><li><p>perpendicular obstacles can be mapped to vertical lines</p><ul><li>pixel intensity  represents  the width of obstacles</li></ul></li><li><p>the traversable region modelled as a succession of planes can be projected as slanted line segment</p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Matching-range-constrained-real-time-loop-closure-detection-with-CNNs-features&quot;&gt;&lt;a href=&quot;#Matching-range-constrained-real-time-loop-
      
    
    </summary>
    
      <category term="SLAM" scheme="http://yoursite.com/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>さようならノート</title>
    <link href="http://yoursite.com/2019/02/17/%E6%96%AD%E7%AB%A0/"/>
    <id>http://yoursite.com/2019/02/17/断章/</id>
    <published>2019-02-17T15:39:00.000Z</published>
    <updated>2019-03-10T06:47:03.911Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/02/17/断章/1.jpg" alt="Cry"></p><p>中断することが喜んでください。<br>中断された心の思い出をマッチング<br>が、絶望的<br>準備と方法<br>単語を置く必要がありますを聞く時<br>あなたを理解すると思う、<br>それは、知っていない彼は個人の心で持っていたことが判明<br>彼は、奇妙に精通しています。<br>酸性または苦いも前例のないです。<br>内臓に影響を及ぼす<br>心臓に血液を返す<br>突然の中心すべて重いです。<br>したいです<br>氷のメモリ<br>破壊されて<br>風の分散<br>灰に残っています。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/2019/02/17/断章/1.jpg&quot; alt=&quot;Cry&quot;&gt;&lt;/p&gt;
&lt;p&gt;中断することが喜んでください。&lt;br&gt;中断された心の思い出をマッチング&lt;br&gt;が、絶望的&lt;br&gt;準備と方法&lt;br&gt;単語を置く必要がありますを聞く時&lt;br&gt;あなたを理解すると思
      
    
    </summary>
    
      <category term="Feeling Notes" scheme="http://yoursite.com/categories/FEELING-NOTES/"/>
    
    
      <category term="Informal essay" scheme="http://yoursite.com/tags/INFORMAL-ESSAY/"/>
    
  </entry>
  
  <entry>
    <title>Five paper about SSL</title>
    <link href="http://yoursite.com/2019/02/17/SSL/"/>
    <id>http://yoursite.com/2019/02/17/SSL/</id>
    <published>2019-02-17T11:00:41.000Z</published>
    <updated>2019-02-17T16:47:20.219Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Zero-Shot-Action-Recognition-with-Error-Correcting-Output-Codes"><a href="#Zero-Shot-Action-Recognition-with-Error-Correcting-Output-Codes" class="headerlink" title="Zero-Shot Action Recognition with Error-Correcting Output Codes"></a>Zero-Shot Action Recognition with Error-Correcting Output Codes</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>主旨</p><ul><li>本文从采用零样本纠错输出码（ZSECOC）的角度来探索ZSAR(Zero-shot action recognition零样本动作识别)</li><li>文中的提出的零样本纠错输出码（ZSECOC）与传统纠错输出码（ECOC）不同之处在于， 为传统ECOC提供了ZSAR的能力。 </li></ul></li><li><p>方法</p><ul><li>从类级语义和内在数据结构中学习出区分可见类别的ZSECOC 。</li><li>通过将可见类别之间已确定的相关性转移到不可见类别之间来隐式地处理域转移</li><li>开发了一种简单的语义转移策略，用于显式地转换已学习的可见类别嵌入，以更好地适应不可见类别的底层结构。</li></ul></li><li><p>优点</p><ul><li>ZSECOC既继承了ECOC的优良特性，又克服了域偏移的问题，使其对ZSAR具有更好的识别能力。</li></ul></li><li><p>评测</p><ul><li>系统地评估了ZSECOC的三个现实行动基准，即奥林匹克运动、HMDB51和UCF101</li><li>实验结果清楚地表明了ZSECOC方法优于目前最先进的方法。</li></ul></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>情景</p><ul><li>健壮的动作识别通常依赖于大量标记的训练示例。然而，在许多实际的场景中，为不断增长的新类别添加足够的示例是非常不适用的</li></ul></li><li><p>目的</p><ul><li>开发一个能够自动识别来自新/不可见类别的操作的动作识别系统。</li></ul></li><li><p>调研方法</p><ul><li><p>零距离学习（Zero-shot learning ,ZSL）</p><ul><li>可以通过标签嵌入(或称为中间表示)来实现，其中语义属性得到了广泛的应用。</li><li>然而，属性通常是手工指定的，而且非常主观，因为它们要么是启发式定义的，要么是由领域专家提供的。特别是对于零样本动作识别(zero-shot action recognition, ZSAR)，基于属性的识别方法存在一些具体的缺陷。</li><li><p>缺陷</p><ul><li>首先，动作通常由“动词”定义，它们缺乏定义良好的类层次关系。其次，动态操作比对象更复杂，因此很难为不同的操作指定合适的属性池。</li></ul></li></ul></li><li><p>字嵌入（ word embeddings）</p><ul><li>通过使用来自大型文本语料库(例如wikipedia)的单词向量，我们只需要类别名称来构建标签嵌入，而不需要耗时的手动指定属性。</li><li><p>缺陷</p><ul><li>然而，嵌入空间的维数m通常较高(通常为m &gt;1000)，因此对于需要训练m个可视化语义映射函数(即从视觉特征到标签嵌入的投影)。</li><li>此外，词向量只考虑类别名称的文本分布式表示，没有考虑原始的可视化数据结构。这将直接导致最终ZSAR的识别能力较差。</li></ul></li></ul></li><li><p>期望的方法ECOC</p><ul><li>因此，我们非常希望寻找一种有区别的、可伸缩的、可以绕过上述缺陷的标签嵌入。通过仔细研究ZSAR的本质，我们发现我们的目标直观上等同于设计分类级纠错输出代码(ECOC)。</li><li><p>优点</p><ul><li><p>Error-correcting abilities.</p><ul><li>通过使用一些冗余位，我们可以容忍一定程度的错误1。利用这一特性可以增强ZSAR的鲁棒性。</li></ul></li><li><p>High efficiency.</p><ul><li>只需少量位元，二进制码匹配速度极快，可实现大规模的ZSAR。</li></ul></li><li><p>Accurate binary classification for each bit.</p><ul><li>这可能导致可靠的可视化语义映射。</li></ul></li></ul></li><li><p>缺陷</p><ul><li>然而，以往的ECOC研究大多针对多类分类，对ZSL的研究较少。这可能是因为直接使用训练在可见类别上的分类器来预测不可见实例将导致较差的性能(称为域移位[23])</li></ul></li></ul></li></ul></li><li><p>本文方法</p><ul><li>具体地说，我们从从大规模文本语料库中获取的分类级语义关联中推导出判别ZSECOC，即谷歌新闻(≈1000亿字)。</li><li>类别之间的语义关联就像一条隧道，将重要的知识从可见的类别隐式地转移到不可见的类别，例如，未知的“三级跳”可以从“跳高”和“跳远”中学习。</li><li>这种知识转移可以在一定程度上解决领域转移问题。在设计判别ZSECOC时，除了保留语义外，还考虑了视觉数据固有的局部结构。</li><li>此外，与需要从不可见类别中访问可视数据的转导方法[23,59,60]不同，我们开发了一种不使用任何不可见数据的简单语义转移策略，为不可见类别生成有效的ZSECOC。</li><li>这种策略显式地转换了可见类别的学习嵌入，以更好地适应不可见类别的底层语义结构。这样可以进一步消除域偏移的影响。</li></ul></li><li><p>本文主要贡献</p><ul><li><p>1</p><ul><li>通过设计有区别的ZSECOC来解决ZSAR问题。我们利用定义良好的类层次关系的词向量，通过发现所见类别之间的语义相关性，定量地度量它们，从而使传统的ECOC具备了ZSAR的能力。</li><li>已建立的语义知识进一步转移到语义相关的无形范畴。因此，提出的ZSECOC既继承了ECOC的固有优势，又克服了域偏移的问题。</li></ul></li><li><p>2</p><ul><li>除了保留类别级语义外，我们的ZSECOC还包含实例级可视数据结构。针对这一问题，提出了一种联合优化框架。高质量的ZSECOC是通过高效的离散优化直接学习的，没有任何松弛。</li></ul></li><li><p>3</p><ul><li>针对奥运会运动项目[39]、HMDB51[24]和UCF101[53]这三个真实的视频动作数据集，对提出的ZSECOC进行了系统的评价。</li><li>就ZSAR而言，最先进的性能清楚地展示了我们方法的优越性。</li></ul></li></ul></li></ul><h2 id="Zero-shot-Learning-Using-Synthesised-Unseen-Visual-Data-with-Diffusion-Regularisation"><a href="#Zero-shot-Learning-Using-Synthesised-Unseen-Visual-Data-with-Diffusion-Regularisation" class="headerlink" title="Zero-shot Learning Using Synthesised Unseen Visual Data with Diffusion Regularisation"></a>Zero-shot Learning Using Synthesised Unseen Visual Data with Diffusion Regularisation</h2><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>背景</p><ul><li>零距离学习(Zero-shot Learning, ZSL)利用视觉属性或自然语言语义作为中间层线索，将低层特征与高层类关联起来，这是该思想的一个新颖扩展</li></ul></li><li><p>目标</p><ul><li>我们的目标是仅使用语义属性来合成新类的训练数据。</li></ul></li><li><p>挑战</p><ul><li>首先，如何防止合成数据过度拟合到训练组?</li><li>其次，如何保证合成的数据对ZSL任务具有鉴别性?</li><li>第三，我们观察到只有少数维度的学习特征获得高的方差，而其余的大部分维度没有提供信息。</li></ul></li><li><p>如何解决</p><ul><li>问题是如何使集中的信息扩散到合成数据的大部分维度。</li><li>提出了一种新的嵌入算法，即不可见视觉数据合成(UVDS)算法，该算法将语义特征投射到高维视觉特征空间中。</li><li><p>在我们提出的算法中引入了两种主要的技术。</p><ul><li>我们引入了一个潜在的嵌入空间，旨在调和视觉空间和语义空间的结构差异，同时保留局部结构。</li><li>我们提出了一种新的扩散正则化(DR)，它明确地迫使方差扩散到合成数据的大多数维度上。通过正交旋转(更准确的说是正交变换)，DR可以去除冗余的相关属性，进一步缓解过拟合问题。</li></ul></li><li><p>效果</p><ul><li>在四个基准数据集上，我们展示了使用合成的不可见数据进行零距离学习的好处。大量的实验结果表明，我们提出的方法明显优于最先进的方法。</li></ul></li></ul></li></ul><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>ZSL</p><ul><li>利用一组封闭的语义模型，这些模型可以泛化成越来越多的新类[1]、[2]、[3]、[4]。因为语义信息可以通过人类知识获得，所以可以动态地创建新类，而不需要收集任何新的可视化数据。</li><li>共同范式的灵感来自于人类仅仅通过了解概念描述就可以识别新事物，因为我们可以将概念与我们之前的知识联系起来。</li><li><p>遵循这种思想，ZSL的第一步是训练一个可以将可视化数据映射到语义表示的预测模型。</p><ul><li>前者开发了旨在从视觉数据准确预测人类知识的高级模型，如概率模型DAP和IAP</li></ul></li><li><p>此后，只要知道新类别的语义描述，就可以识别它们。现有的ZSL研究分为两大主流:预测模型和语义表示设计。</p></li></ul></li><li><p>研究近况</p><ul><li>最近的研究利用嵌入方法作为低层特征和类标签之间的中间层。</li><li>此外，一些新颖的著作研究了如何直接构造不可见类的分类器。</li><li>后一种流侧重于如何有效地表示可以归纳为新类的人类知识，如人类可命名属性、词向量、文本描述以及类相似性。</li></ul></li><li><p>方法不足</p><ul><li>上述方法存在一个共同的不足，即在语义信息不断增加、新类不断添加的情况下，训练的可视化示例无法扩展。由于新概念不断增长，这是不可避免的</li></ul></li><li><p>本文的方法</p><ul><li>在这篇论文中，我们提议对不可见类的训练数据进行合成。我们的想法是受到人类想象力的启发。</li><li>给出一个语义描述，人类可以将熟悉的视觉元素联系起来，然后想象一个近似的场景。</li><li>值得注意的是，我们的方法不同于[1]中的图像合成，因为从语义上合成的图像很难覆盖视觉表象的巨大变化。取而代之的是，我们合成有区别的低级特征来训练ZSL的监督分类器。</li><li>这种方法在ZSL任务和传统的监督分类器之间提供了一个直接的接口。</li><li>此外，它还支持高级概念和低级可视特性之间的信息交互流。这样，训练集可以扩展到与语义表示一样大</li></ul></li><li><p>面临的技术难题</p><ul><li><p>首先是视觉语义的差异。</p><ul><li>由于提取的数据源和方法的视觉特征和语义特征不同，这两个数据空间的数据分布可能存在显著差异。一个空间中的两个闭合点在另一个空间中可能很远。例如，[23]报道，同样的属性“HasTail”可能在“Zebra”和“Pig”的视觉外观上有很大的区别。然而，我们希望该模型能够有效捕获语义-视觉关联，而不是针对[23]中识别任务的“域转移问题”</li></ul></li><li><p>第二个问题是方差衰减。</p><ul><li>由于视觉特征维数通常远大于语义表征维数，学习投影容易出现不平衡，即投影维数的方差变化严重。如图6所示，与真实数据相比，我们观察到线性投影合成的数据存在显著的方差衰减。大多数投影维数的方差都非常小，说明它们获得的信息很少。由于大量的冗余维度，这种预测会导致性能下降。因此，挑战在于如何使信息以平衡的投影扩散到合成数据的大部分维度。据我们所知，这个问题在之前的ZSL文献中没有被发现</li></ul></li></ul></li><li><p>提出的方法</p><ul><li>提出了一种新的嵌入算法，即不可见视觉数据合成(UVDS)算法，该算法将语义特征投射到高维视觉特征空间中。</li><li>对于第一个问题，我们引入了一个潜在的嵌入空间来调和视觉空间和语义空间之间的结构差异。我们使用双图(GR)来保持视觉和语义空间的局部结构。</li><li>对于第二个问题，我们提出了一个新的扩散正则化(DR)，它明确地使信息扩散到合成数据的所有维度。具体地说，我们使用方差作为测量来强制信息在合成数据的维度上扩散。</li><li>我们证明了这种格式等价于找到正交旋转变换。同时，我们还发现了一种优雅的正交旋转形式，它使用了有效解的2,1范数正则化。</li><li>除了上述两个问题，合成的数据对ZSL任务也应该是有区别的。直接回归模型倾向于学习两个空间之间的主成分，这导致了对训练集分类的高度偏差。我们认为这是一个过拟合的问题，即训练后的模型在可见类的合成数据上可以获得较高的性能，但在合成的不可见数据上性能会显著下降。</li><li>我们的经验表明，上述GR和DR可以互补地缓解过拟合问题:DR不损害局部结构的保留，而是通过正交旋转消除语义空间中的冗余相关性，从而有利于数据的合成。</li></ul></li><li><p>贡献</p><ul><li>一个直观的框架，使我们能够从给定的语义属性合成不可见的数据。</li><li>合成的数据可以直接提供给典型的分类器，并导致在四个基准数据集的最先进的性能。</li><li>一种新的扩散正则化，可以显式地使信息扩散到合成数据的各个维度。我们通过优化正交旋转问题来实现信息扩散。我们提供了一种有效的优化策略来解决这一问题，同时数据结构的保存和数据重建</li></ul></li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://blog.sciencenet.cn/blog-205121-1114916.html" target="_blank" rel="noopener">http://blog.sciencenet.cn/blog-205121-1114916.html</a></li></ul><h2 id="Prototypical-Networks-for-Few-shot-Learning"><a href="#Prototypical-Networks-for-Few-shot-Learning" class="headerlink" title="Prototypical Networks for Few-shot Learning"></a>Prototypical Networks for Few-shot Learning</h2><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>目的</p><ul><li>针对小波分类问题，我们提出了原型网络，其中分类器必须推广到训练集中没有出现的新类，只给出每个新类的少量示例。</li><li>原型网络学习一个度量空间，在这个空间中，可以通过计算到每个类的原型表示的距离来执行分类。与近年来的少镜头学习方法相比，它们反映了一种更简单的归纳偏差，在这种有限的数据体制下是有益的，并取得了良好的效果。</li></ul></li><li><p>结果</p><ul><li>我们提供的分析表明，一些简单的设计决策相对于最近涉及复杂架构选择和元学习的方法可以产生实质性的改进。我们进一步将原型网络扩展到零距离学习，并在CU-Birds数据集上实现了最先进的结果</li></ul></li></ul><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>小样本分类</p><ul><li>few -shot classification[22,18,15]是一项任务，其中分类器必须进行调整，以适应在培训中没有看到的新类，只给出每个类的几个示例。一个简单的方法，例如在新数据上重新训练模型，将会严重地过度拟合。虽然这个问题相当困难，但已经证明人类有能力执行哪怕是一次分类，即只给出每个新类的一个示例，并且具有很高的准确率[18]。</li></ul></li><li><p>近期两种方法</p><ul><li><p>1</p><ul><li>Vinyals et al.[32]提出了匹配网络，该网络使用一种注意力机制来预测未标记点(查询集)的类。<br>匹配网络可以解释为在嵌入空间中应用的加权最近邻分类器。</li><li>值得注意的是，该模型利用了训练过程中被称为插曲的抽样小批量，其中每一集都被设计成通过子抽样类和数据点来模拟少数镜头任务。章节的使用使得训练问题更加忠实于测试环境，从而提高了泛化能力。</li></ul></li><li><p>2</p><ul><li>Ravi和Larochelle[24]进一步提出了情景式训练的概念，并提出了一种元学习的方法来实现少镜头学习。他们的方法包括培训</li><li>LSTM[11]生成对分类器的更新，给定一个集，这样就可以很好地推广到一个测试集。</li><li>在这里，LSTM元学习者不是在多个情景中训练单个模型，而是学习为每个情景训练一个定制的模型。</li></ul></li></ul></li><li><p>本文的方法的思想</p><ul><li>我们通过解决过拟合的关键问题来解决小批量学习的问题。由于数据非常有限，我们假设分类器应该有一个非常简单的归纳偏差。我们的方法，原型网络，是基于这样一种思想，即存在一种嵌入，其中点围绕每个类的单一原型表示聚类。为了做到这一点，我们使用神经网络学习了输入到嵌入空间的非线性映射，并将类的原型作为其在嵌入空间中支持集的平均值。</li><li>我们采用同样的方法来处理零距离学习;在这里，每个类都带有元数据，提供了类的高级描述，而不是少量标记的示例。因此，我们学习将元数据嵌入到共享空间中，作为每个类的原型。</li><li>就像在少数情况下那样，通过为嵌入式查询点查找最近的类原型来执行分类</li></ul></li><li><p>本文方法</p><ul><li>在这篇论文中，我们建立了原型网络的两种设置，少拍和零拍。</li><li>我们在一次设置中绘制匹配网络的连接，并分析模型中使用的底层距离函数。特别地，我们将原型网络与聚类[4]联系起来，以证明在使用Bregman散度计算距离(如平方欧氏距离)时使用类均值作为原型是合理的。</li><li>我们从经验上发现，距离的选择是至关重要的，因为欧几里得距离远远优于更常用的余弦相似度。在几个基准任务上，我们实现了最先进的性能。</li><li>与最近的元学习算法相比，原型网络更简单、更有效，这使得它们成为一种吸引人的少目标和零目标学习方法</li></ul></li></ul><h3 id="参考资料-1"><a href="#参考资料-1" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://blog.csdn.net/u014767662/article/details/81670215" target="_blank" rel="noopener">https://blog.csdn.net/u014767662/article/details/81670215</a></li></ul><h2 id="A-multimodal-cortical-network-for"><a href="#A-multimodal-cortical-network-for" class="headerlink" title="A multimodal cortical network for"></a>A multimodal cortical network for</h2><p>the detection of changes in the<br>sensory environment</p><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>引入</p><ul><li>经历突然变化的感官刺激会吸引注意力，并优先进入我们的意识。</li><li>我们使用事件相关的功能性磁共振成像(fMRI)来识别大脑中对视觉、听觉和触觉刺激变化做出反应的区域。单模反应区包括视觉、听觉和躯体感觉联合皮层。</li><li>多模态反应区包括颞顶叶交界处、额下回、岛叶、左扣带回和辅助运动区等右脑网络。这些结果揭示了一个分布式的多模态网络，用于无意识地注意感官环境中的事件。</li><li>该网络包含被认为是P300事件相关电位基础的区域，与半eglect综合征患者受损的皮层区域密切相关。</li></ul></li></ul><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>引入</p><ul><li>感知环境变化的能力对生存至关重要。有必要关注这些变化，以评估和修改在面临发展障碍、机会或威胁时的行为。因此，感官环境的变化，尤其是突然的变化，往往会不由自主地引起注意。经历变化的感官元素也优先地将自身插入意识中。例如，一个徒步旅行者可能不会注意到持续不断的鸟鸣声，除非他们突然停止，这时，徒步旅行者会意识到这两种声音</li><li>当处理感官世界刺激的能力丧失时，就像患有忽视综合症的患者一样，对刺激的意识也丧失了1,2。理解大脑探测感觉环境变化的机制，将有助于我们更好地理解无意识注意和意识的机制。</li><li>我们使用事件相关功能核磁共振成像来识别神经解剖结构的网络，这一网络是检测感觉环境变化的基础。视觉、听觉和触觉刺激被用来识别对多种感觉模式变化作出反应的区域。这些多模态区域特别有助于理解高阶认知过程，如构建一个完整的、多感官感知环境、将注意力引向该环境的显著特征以及选择这些特征以进入意识</li></ul></li><li><p>实验</p><ul><li>受试者在接受视觉、听觉和触觉刺激的同时接受fMRI检查。为避免因反应选择、计划或工作记忆而激活，实验过程中不要求受试者做出任何形式的反应。相反，他们只是被动地观察刺激。</li><li>关于刺激事件检测的研究经常涉及到</li><li>“古怪”的实验方案，在这个方案中，研究对象面对一系列重复的、标准的刺激物，这些刺激物偶尔会被不同的刺激物打断</li><li>“古怪”的刺激。我们的研究使用了这种方法的修正版本。在我们的方案中，每一种刺激方式都是连续呈现的，但在两种不同的状态(A和b)之间是独立交替的</li><li>A到B或B到A(图1)</li><li>我们使用这些抵消刺激状态之间的转换,而不是一个典型的古怪的刺激,以确保激活是由于一般质量的刺激和变化并不仅仅是由于不同的一些具体特征古怪的刺激与标准相比。每14秒就有3种感官模式中的一种发生转换，以随机顺序排列，以最小化预期和习惯的影响。</li><li>为了识别激活，我们将转换视为刺激事件。这种方法使我们能够识别在单一感觉模式下对转换有反应的皮层区域，以及在多种感觉模式下对转换有反应的皮层网络</li></ul></li></ul><h2 id="Are-GANs-Created-Equal-A-Large-Scale-Study"><a href="#Are-GANs-Created-Equal-A-Large-Scale-Study" class="headerlink" title="Are GANs Created Equal? A Large-Scale Study"></a>Are GANs Created Equal? A Large-Scale Study</h2><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>生成对抗网络</p><ul><li>生成对抗网络(GAN)是生成模型的一个强大子类。尽管有非常丰富的研究活动导致许多有趣的</li><li>GAN算法，仍然很难评估哪种算法比其他算法表现得更好。我们对先进的模型和评价措施进行了中立的、多方面的大规模实证研究。</li></ul></li><li><p>分析</p><ul><li>我们发现，大多数模型都可以达到类似的分数，只要有足够的超参数优化和随机重启。</li><li>这表明改进可以来自更高的计算预算和比基本算法更改更多的调优。</li></ul></li><li><p>困难</p><ul><li>为了克服当前度量标准的一些局限性，我们还提出了几个可以计算精确度和召回率的数据集。我们的实验结果表明，今后的GAN研究应该建立在更加系统和客观的评价程序的基础上。</li></ul></li><li><p>方法</p><ul><li>最后，我们没有发现任何经过测试的算法始终优于[9]中引入的非饱和GAN的证据。</li></ul></li></ul><h3 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>本文贡献</p><ul><li><p>1</p><ul><li>我们提供了一个公平和全面的最先进的比较</li><li>GANs和经验表明，如果有足够高的计算预算，它们几乎都可以达到FID的类似值</li></ul></li><li><p>2</p><ul><li>We provide strong empirical evidence2 that to compare GANs</li><li>it is necessary to report a summary of distribution of results, rather than the best result achieved, due</li><li>to the randomness of the optimization process and model instability. (</li></ul></li><li><p>3</p><ul><li>我们评估FID对模式下降的鲁棒性，使用不同的编码网络，并提供最佳的估计</li><li>FID可以在经典数据集上实现。</li></ul></li><li><p>4</p><ul><li>我们介绍了一系列难度越来越大的任务，可以对这些任务进行近似计算，如精度和召回等。</li></ul></li><li><p>5</p><ul><li>开源实现</li></ul></li></ul></li></ul><h3 id="参考资料-2"><a href="#参考资料-2" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://www.sohu.com/a/207548482_465975" target="_blank" rel="noopener">http://www.sohu.com/a/207548482_465975</a></li></ul><h2 id="Few-shot-learning-of-neural-networks-from"><a href="#Few-shot-learning-of-neural-networks-from" class="headerlink" title="Few-shot learning of neural networks from"></a>Few-shot learning of neural networks from</h2><p>scratch by pseudo example optimization</p><h3 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>本文提出了一种简单有效的训练神经网络的方法。我们的方法继承了知识精馏的思想，即将知识从深度或广泛的参考模型转移到浅层或狭窄的目标模型。</li><li>该方法利用这一思想来模拟参考估计量的预测，这些估计量比我们想要训练的网络更能抵抗过拟合。与以往几乎所有需要大量标记训练数据的知识提取工作不同，该方法只需要少量训练数据。</li><li>相反，我们引入了作为模型参数的一部分进行优化的伪训练示例。对多个基准数据集的实验结果表明，该方法优于目标模型的朴素训练和标准知识提取等所有基线。</li></ul><h3 id="Introduction-5"><a href="#Introduction-5" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>深度学习过拟合的挑战</li><li><p>本文</p><ul><li>本文提出了一种利用少量有监督训练数据对神经网络进行训练的新方法。图1展示了我们所提出的模仿网络方法的基本思想。</li><li>在原理上，我们特别选择能够提供局部平滑预测的GPs作为参考模型。与以往几乎所有使用大量监督训练样本进行知识精化的工作不同，我们提出的方法只需要少量的监督训练样本进行知识转移。为了增加训练示例，我们引入了一些诱导点[30]，这些点是伪训练示例，可以帮助模型训练变得易于处理或简单得多。在原始的诱导点方法中，采用了可伸缩GP推理的诱导点和模型参数</li><li>然而，在我们提出的方法中，目标模型的参数被更新以减少训练损失，而伪训练实例被更新以增加训练损失。通过这样做，我们可以将伪训练示例移到当前目标模型没有得到良好训练的区域。我们还引入了保真度加权[6]，用于消除基于参考模型预测不确定性的有害伪训练实例</li></ul></li><li><p>主要贡献</p><ul><li><ol><li>提出了一种新的神经网络从无到有小概率训练的框架，这意味着既不需要额外的例子也不需要用大量的监督例子训练参考模型。</li></ol></li><li><ol start="2"><li>将诱导点的思想应用到神经网络的训练中，其优化方法与神经网络的模型参数几乎相同。</li></ol></li></ul></li></ul><p><em>XMind: ZEN - Trial Version</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Zero-Shot-Action-Recognition-with-Error-Correcting-Output-Codes&quot;&gt;&lt;a href=&quot;#Zero-Shot-Action-Recognition-with-Error-Correcting-Output
      
    
    </summary>
    
      <category term="Reading Notes of Paper" scheme="http://yoursite.com/categories/READING-NOTES-OF-PAPER/"/>
    
    
      <category term="SSL" scheme="http://yoursite.com/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>Day One</title>
    <link href="http://yoursite.com/2017/11/25/Day-One/"/>
    <id>http://yoursite.com/2017/11/25/Day-One/</id>
    <published>2017-11-25T06:54:13.000Z</published>
    <updated>2017-11-25T07:51:54.260Z</updated>
    
    <content type="html"><![CDATA[<p>今天是11月25号，已经颓靡了很久的我终于搭建了一个可以访问的博客了，使用过阿里云的学生机，也使用过world press+老薛主机，两次都购买了一个月的使用期，配置过程中也遇到了很多问题，往往问题还没有解决的时候空间就要过期了。<br> 昨天尝试了使用hexo+github的免费静态网页搭建，虽然也遇到了坑，但是也花了一下午就搭建完成可以使用域名访问了，成就感真的，哇!终于有了自己的站点了。<br> 话说回来，一开始想用阿里云和老薛主机建站的目的，一方面是为了督促自己学习，一方面是为了通过服务器搭建网站帮助自己熟悉前后端的知识，可惜没有坚持下去。<br> 所以希望现在这个静态的免费站点能够支撑我记录自己的成长，不要荒废了我的时光和岁月！！！<br> 长久没有读书，写的文字也没有逻辑性，希望自己变成自己想要变成的样子！<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;--more--&gt;</span><br><span class="line"> 加油~~</span><br></pre></td></tr></table></figure></p><p>对话框么么？<br>~~~~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天是11月25号，已经颓靡了很久的我终于搭建了一个可以访问的博客了，使用过阿里云的学生机，也使用过world press+老薛主机，两次都购买了一个月的使用期，配置过程中也遇到了很多问题，往往问题还没有解决的时候空间就要过期了。&lt;br&gt; 昨天尝试了使用hexo+githu
      
    
    </summary>
    
    
  </entry>
  
</feed>
